b storj decentralized cloud storage network framework storj labs inc october 30 2018 v3 0 https github com storj whitepaper x0c2 copyright xc2 xa9 2018 storj labs inc subsidiaries work licensed creative commons attribution sharealike 3 0 license cc sa 3 0 product names logos brands cited document property respective owners company product service names identification purposes use names logos brands imply endorsement x0ccontents 0 1 abstract 6 0 2 contributors 6 1 introduction 7 2 storj design constraints 9 2 1 security privacy 9 2 2 decentralization 9 2 3 marketplace economics 10 2 4 amazon s3 compatibility 12 2 5 durability device failure churn 12 2 6 latency 13 2 7 bandwidth 14 2 8 object size 15 2 9 byzantine fault tolerance 15 2 10 coordination avoidance 16 3 framework 18 3 1 framework overview 18 3 2 storage nodes 19 3 3 peer peer communication discovery 19 3 4 redundancy 19 3 5 metadata 23 3 6 encryption 24 3 7 audits reputation 25 3 8 data repair 25 3 9 payments 26 x0c4 4 concrete implementation 27 4 1 definitions 27 4 2 peer classes 30 4 3 storage node 31 4 4 node identity 32 4 5 peer peer communication 33 4 6 node discovery 33 4 7 redundancy 35 4 8 structured file storage 36 4 9 metadata 39 4 10 satellite 41 4 11 encryption 42 4 12 authorization 43 4 13 audits 44 4 14 data repair 45 4 15 storage node reputation 47 4 16 payments 49 4 17 bandwidth allocation 50 4 18 satellite reputation 53 4 19 garbage collection 53 4 20 uplink 54 4 21 quality control branding 55 5 walkthroughs 56 5 1 upload 56 5 2 download 58 5 3 delete 58 x0c5 5 4 59 5 5 copy 60 5 6 list 60 5 7 audit 61 5 8 data repair 61 5 9 payment 62 6 future work 63 6 1 hot files content delivery 63 6 2 improving user experience metadata 64 7 selected calculations 65 7 1 object repair costs 65 7 2 audit false positive risk 67 7 3 choosing erasure parameters 69 distributed consensus 73 b attacks 76 c primary user benefits 79 x0ccontents 0 1 6 abstract decentralized cloud storage represents fundamental shift efficiency economics large scale storage eliminating central control allows users store share data reliance party storage provider decentralization mitigates risk data failures outages simultaneously increasing security privacy object storage allows market forces optimize expensive storage greater rate single provider afford ways build system specific responsibilities given implementation address based experience petabyte scale storage systems introduce modular framework considering responsibilities building distributed storage network additionally describe initial concrete implementation entire framework 0 2 contributors paper represents combined efforts individuals contributors affiliated storj labs inc include limited tim adams kishore aligeti cameron ayer atikh bana alexander bender stefan benten maximillian von briesen paul cannon gina cooley dennis coyle egon elbre nadine farah patrick gerbes john gleeson ben golub james hagans jens heimb xc3 xbcrge faris huskovic philip hutchins brandon iglesias viktor ihnatiuk jennifer johnson kevin leffew alexander leitner richard littauer dylan lott jt olio kaloyan raev garrett ransom matthew robinson jon sanderson benjamin sirb dan sorensen helene unland natalie villasana bryan white shawn wilkinson xe2 x80 x99d like thank authors contributors previous storj metadisk white papers tome boshevski josh brandoff vitalik buterin braydon fuller gordy hall jim lowry chris pollard james prestwich xe2 x80 x99d like especially thank petar maymounkov anand babu periasamy tim kosse roberto galoppini steven willoughby aaron boodman helpful review contributions early draft paper like acknowledge efforts white papers communications distributed computing blockchain distributed storage decentralized storage space work informed efforts comprehensive list sources bibliography like provide particular acknowledgement guidance inspiration provided teams designed built allmydata ceph coralcdn ethereum farsite filecoin freenet gluster gfs hadoop ipfs kademlia lustre maidsafe minio mojonation oceanstore scality siacoin tahoe lafs finally extend huge thank talked design architecture system valuable thoughts feedback input suggestions address correspondence paper storj io x0c1 introduction internet massive decentralized distributed network consisting billions devices controlled single group entity data currently available internet centralized stored handful technology companies experience capital build massive data centers capable handling vast information challenges faced data centers data breaches periods unavailability grand scale storage costs expanding upgrading quickly meet user demand faster data larger formats decentralized storage emerged answer challenge providing performant secure private economical cloud storage solution decentralized storage better positioned achieve outcomes architecture natural alignment decentralized architecture internet opposed massive centralized data centers news coverage data breaches past years shown frequency breaches increasing factor 10 2005 2017 1 decentralized storage xe2 x80 x99s process protecting data makes data breaches difficult current methods data centers time costing current storage methods model address rapidly expanding data current solutions struggle anticipated 44 zettabytes data expected exist 2020 market grow $92 billion usd time frame 2 identified key market segments decentralized cloud storage potential address decentralized cloud storage capabilities evolve able address wider range use cases basic object storage content delivery networks cdn decentralized cloud storage rapidly advancing maturity evolution subject specific set design constraints define overall requirements implementation network designing distributed storage system parameters optimized speed capacity trustlessness byzantine fault tolerance cost bandwidth latency propose framework scales horizontally exabytes data storage globe system storj network robust object store encrypts shards distributes data nodes world storage data stored served manner purposefully designed prevent breaches order accomplish task xe2 x80 x99ve designed system modular consisting independent components taskspecific jobs xe2 x80 x99ve integrated components implement decentralized object storage system secure performant reliable significantly economical premise traditional centralized cloud storage x0cchapter 1 introduction 8 organized rest paper additional chapters chapter 2 discusses design space storj operates specific constraints optimization efforts based chapter 3 covers framework chapter 4 proposes simple concrete implementation framework chapter 5 explains happens operation network chapter 6 covers future work finally chapter 7 covers selected calculations x0c2 storj design constraints designing system xe2 x80 x99s important define requirements different ways design decentralized storage system addition requirements potential design space shrinks significantly design constraints heavily influenced product market fit goals carefully considering requirement ensure framework choose universal possible given constraints 2 1 security privacy object storage platform ensure privacy security data stored regardless centralized decentralized decentralized storage platforms mitigate additional layer complexity risk associated storage data inherently untrusted nodes decentralized storage platforms shortcuts data center based approaches e g firewalls dmzs etc decentralized storage designed ground support end toend encryption enhanced security privacy levels system certain categories data subject specific regulatory compliance example united states legislation health insurance portability accountability act hipaa specific requirements data center compatibility european countries consider general data protection regulation gdpr individual information protected secured customers outside united states feel significant geopolitical reasons consider storing data way limits ability based entities impact privacy 3 regulations sectors user xe2 x80 x99s data privacy customers able evaluate software implemented correctly resistant attack vectors known unknown secure fulfills customers xe2 x80 x99 requirements open source software provides level transparency assurance needed prove behaviors system advertised 2 2 decentralization informally decentralized application service single operator furthermore single entity solely responsible cost associated running service able cause service interruption users main motivations preferring decentralization drive infrastructure costs maintenance utilities bandwidth believe significant underutilized resources edge network smaller operators ex x0cchapter 2 storj design constraints 10 perience building decentralized storage networks found long tail resources presently unused underused provide affordable geographically distributed cloud storage conceivably small operator access expensive electricity standard data centers small operator access expensive cooling small operator environments substantial run entire datacenter like storage system example small business home network attached storage nas operator excess electricity run hard drives found aggregate small operator environments exist combination internet constitutes significant opportunity advantage expensive faster storage decentralization goals fundamental infrastructure storage driven desire provide viable alternative major centralized storage entities dominate market present believe exists inherent risk trusting single entity company organization significant percentage world xe2 x80 x99s data fact believe implicit cost associated risk trusting party custodianship personal data possible costly outcomes include changes company xe2 x80 x99s roadmap result product useful changes company xe2 x80 x99s position data collection cause sell customer metadata advertisers company business fail customer data safe creating equivalent better decentralized system users concerned single entity risk viable alternative decentralized architecture storj cease operating data continue available decided adopt decentralized architecture despite tradeoffs believe decentralization better addresses needs cloud storage resolves core limitations risks cost factors result centralization context decentralization results globally distributed network serve wide range storage use cases archival cdn centralized storage systems require different architectures implementations infrastructure address use cases 2 3 marketplace economics public cloud computing public cloud storage particular proven attractive business model large centralized cloud providers cloud computing estimated $186 4 billion dollar market 2018 expected reach $302 5 billion 2021 4 public cloud storage model provided compelling economic model end users enable end users scale demand allows avoid significant fixed costs facilities power data center personnel public x0cchapter 2 storj design constraints 11 cloud storage generally proven economical durable performant option end users compared premise solutions public cloud storage model nature led high degree concentration fixed costs born network operators invest billions dollars building network data centers enjoy significant economies scale combination large upfront costs economies scale means extremely limited number viable suppliers public cloud storage arguably fewer major operators worldwide suppliers primary beneficiaries economic return believe decentralized storage provide viable alternative centralized cloud encourage partners customers bring data network price charged storage bandwidth xe2 x80 x94combined benefits decentralized storage xe2 x80 x94must compelling economically beneficial competing storage solutions design storj seek create economically advantageous situation different groups end users provide economically compelling characteristics public cloud storage upfront costs scale demand addition end users experience meaningfully better value given levels capacity durability security performance storage node operators economically attractive storage node operators help build network paid fairly transparently able reasonable profit relative marginal costs incur economically advantageous storage node operator utilizing underused capacity creating new capacity grow network capacity currently exists node availability reliability large impact network availability cost durability required storage node operators sufficient incentive maintain reliable continuous connections network demand providers economically attractive developers businesses drive customers data storj network design system fairly transparently deliver margin partners believe unique opportunity provide open source software oss companies projects drive thirds public cloud workloads today receiving direct revenue source sustainable revenue network operator sustain continued investment code functionality network maintenance demand generation network operator currently storj labs inc able retain reasonable profit operator maintain profit charging end users public cloud providers margin sharing storage node operators demand providers additionally network able account ensuring efficient timely billing payment processes regulatory compliance tax reporting globally versatile possible payments network robust accom x0cchapter 2 storj design constraints 12 modate types transactions cryptocurrency bank payments forms barter lastly storj roadmap aligned economic drivers network new features changes concrete implementations framework components driven applicability specific object storage use cases relationship features performance price storage bandwidth relative use cases 2 4 amazon s3 compatibility time paper xe2 x80 x99s publication widely deployed public cloud amazon web services 5 amazon web services largest cloud services ecosystem benefit mover advantage amazon xe2 x80 x99s cloud services product amazon simple storage service amazon s3 short public numbers hard come amazon s3 likely widely deployed cloud storage protocol existence cloud storage products provide form compatibility amazon s3 application program interface api architecture objective aggressively compete wider cloud storage industry bring decentralized cloud storage mainstream decentralized cloud storage protocol widely adopted amazon s3 compatibility creates graceful transition path centralized providers alleviating switching costs users achieve storj implementation allows applications previously built amazon s3 work storj minimal friction changes s3 compatibility adds aggressive requirements feature set performance durability bare minimum requires methods described figure 2 1 implemented 1 2 3 4 5 6 7 8 9 10 bucket operations createbucket bucketname deletebucket bucketname listbuckets object operations getobject bucketname objectpath offset length putobject bucketname objectpath data metadata deleteobject bucketname objectpath listobjects bucketname prefix startkey limit delimiter figure 2 1 minimum s3 api 2 5 durability device failure churn storage platform useless functions retrieval platform storage platform valuable careful lose data given x0cchapter 2 storj design constraints 13 presence variety possible failures system system store data high durability negligible risk data loss devices component failure guarantee hard drives fail wear 6 servers providing network access hard drives eventually fail network links die power failures cause havoc sporadically storage media unreliable time data stored redundancy recover individual component failures importantly data left single location indefinitely environment redundancy data maintenance repair replacement lost redundancy considered inevitable system account issues furthermore decentralized systems susceptible high churn rates participants join network leave reasons hardware actually failed instance rhea et al found real world peer peer systems median time participant lasts network ranges hours mere minutes 7 maymounkov et al found probability node staying connected decentralized network additional hour increasing function uptime figure 2 2 8 words nodes online long time likely contribute overall node churn churn caused number factors storage nodes offline hardware software failure intermittent internet connectivity power loss complete disk failure software shutdown removal network churn exists redundancy required greater rate node loss redundancy required bandwidth needed correct operation system fact tight relationship network churn additional redundancy bandwidth availability 9 background bandwidth usage redundancy low network low network churn strong incentive favor long lived stable nodes section 7 3 3 blake et al 9 discussion repair bandwidth varies function node churn 2 6 latency decentralized storage systems potentially capitalize massive opportunities parallelism opportunities include increased transfer rates processing capabilities overall throughput individual network links slow parallelism improve latency individual network link utilized operation latency lower bound overall operation distributed system intended high performance applications continuously aggressively optimize low latency individual process scale system xe2 x80 x99s entire architecture x0cchapter 2 storj design constraints 14 figure 2 2 probability remaining online additional hour function uptime x axis represents minutes y axis shows fraction nodes stayed online x minutes stayed online x 60 minutes source maymounkov et al 8 2 7 bandwidth global bandwidth availability increasing year year unfortunately access highbandwidth internet connections unevenly distributed world users easily access symmetric high speed unlimited bandwidth connections significant difficulty obtaining type access united states countries method residential internet service providers isps operate presents specific challenges designers decentralized network protocol challenge asymmetric internet connections offered isps customers subscribe internet service based advertised download speed upload speed potentially order magnitude slower second challenge bandwidth xe2 x80 x9ccapped xe2 x80 x9d isp fixed allowed traffic month example markets isp comcast imposes terabyte month bandwidth cap stiff fines customers limit 10 internet connection cap 1 tb month average 385 kb s month exceeding monthly bandwidth allowance isp advertises speeds 10 mb s higher caps impose significant limitations bandwidth available network given moment device failure churn guaranteed decentralized system corresponding repair traffic result important account bandwidth required data storage retrieval data maintenance repair 9 designing storage system careless bandwidth usage undue preference storage node operators access unlimited high speed bandwidth centralize system degree order storage system decentralized possible working environments possible bandwidth usage aggressively minimized x0cchapter 2 storj design constraints 15 section 7 1 1 discussion bandwidth availability repair traffic limit usable space 2 8 object size broadly classify large storage systems groups average object size differentiate groups classify xe2 x80 x9clarge xe2 x80 x9d file megabytes greater size database preferred solution storing small pieces information object store file system ideal storing large files initial product offering storj labs designed function primarily decentralized object store larger files future improvements enable database like use cases object storage predominant initial use case described paper protocol design decisions assumption vast majority stored objects 4mb larger smaller files supported simply costly store worth noting negatively impact use cases require reading lots files smaller megabyte users address packing strategy aggregating storing small files large file protocol supports seeking streaming allow users download small files requiring retrieval aggregated object 2 9 byzantine fault tolerance unlike centralized solutions like amazon s3 storj operates untrusted environment individual storage providers necessarily assumed trustworthy storj operates public internet allowing sign storage provider adopt byzantine altruistic rational bar model 11 discuss participants network xe2 x80 xa2 byzantine nodes deviate arbitrarily suggested protocol reason examples include nodes broken nodes actively trying sabotage protocol general byzantine node bad actor optimizes utility function independent given suggested protocol xe2 x80 xa2 inevitable hardware failures aside altruistic nodes good actors participate proposed protocol rational choice deviate xe2 x80 xa2 rational nodes neutral actors participate deviate net best interest distributed storage systems e g datacenter based cloud object storage systems operate environment nodes considered altruistic example absent x0cchapter 2 storj design constraints 16 hardware failure security breaches amazon xe2 x80 x99s storage nodes explicitly programmed amazon owns runs contrast storj operates environment node managed independent operator environment expect majority storage nodes rational minority byzantine storj assumes altruistic nodes include incentives encourage network ensure rational nodes network majority operators behave similarly possible expected behavior altruistic nodes likewise effects byzantine behavior minimized eliminated note creating system robust face byzantine behavior require byzantine fault tolerant consensus protocol xe2 x80 x94we avoid byzantine consensus sections 4 9 6 2 appendix details 2 10 coordination avoidance growing body distributed database research shows systems avoid coordination possible far better throughput systems subcomponents forced coordinate achieve correctness 12 xe2 x80 x9319 use bailis et al xe2 x80 x99s informal definition coordination requirement concurrently executing operations synchronously communicate stall order complete 16 observation happens scales applies distributed networks concurrent threads execution coordinating computer soon coordination needed actors system need wait actors waiting xe2 x80 x94due coordination issues xe2 x80 x94can significant cost types operations network require coordination e g operations require linearizability1 15 20 21 choosing strategies avoid coordination highly available transactions 15 offer performance gains orders magnitude wide area networks fact carefully avoiding coordination possible anna database 17 able 10 times faster cassandra redis corresponding environments 700 800 times faster performance focused memory databases masstree intel xe2 x80 x99s tbb 22 coordination avoided new frameworks invariant confluence 16 calm principle 18 19 allow system architects understand coordination required consistency correctness evidenced anna xe2 x80 x99s performance successes efficient avoid coordination possible systems minimize coordination better scaling small large workloads adding resources coordination avoidant system directly in1 linearizable operations atomic operations specific object order operations equivalent order given original xe2 x80 x9cwall clock xe2 x80 x9d time x0cchapter 2 storj design constraints 17 crease throughput performance adding resources coordinationdependent system bitcoin 23 raft 24 result additional throughput overall performance exabyte scale minimizing coordination key components strategy surprisingly decentralized storage platforms working architectures require significant amounts coordination operations accounted single global ledger achieve exabyte scale fundamental requirement limit hotpath coordination domains small spheres entirely controllable user limits applicability blockchain like solutions use case x0c3 framework having considered design constraints chapter outlines design framework consisting fundamental components framework describes components exist satisfy constraints long design constraints remain constant framework feasible describe storj years design freedom framework framework obviate need future rearchitectures entirely independent components able replaced affecting components 3 1 framework overview designs framework following things store data data stored network client encrypts breaks multiple pieces pieces distributed peers network occurs metadata generated contains information find data retrieve data data retrieved network client reference metadata identify locations previously stored pieces pieces retrieved original data reassembled client xe2 x80 x99s local machine maintain data redundancy drops certain threshold necessary data missing pieces regenerated replaced pay usage unit value sent exchange services rendered improve understandability break design collection independent components combine form desired framework individual components 1 2 3 4 5 6 7 8 storage nodes peer peer communication discovery redundancy metadata encryption audits reputation data repair payments x0cchapter 3 framework 3 2 19 storage nodes storage node xe2 x80 x99s role store return data aside reliably storing data nodes provide network bandwidth appropriate responsiveness storage nodes selected store data based criteria ping time latency throughput bandwidth caps sufficient disk space geographic location uptime history responding accurately audits forth return service nodes paid storage nodes selected changing variables external protocol node selection explicit non deterministic process framework means track nodes selected upload small metadata xe2 x80 x99t select nodes storing data implicitly deterministically system like dynamo 25 gfs 26 hdfs 27 lustre 28 decision implies requirement metadata storage system track selected nodes section 3 5 3 3 peer peer communication discovery peers network communicate standarized protocol framework requires protocol xe2 x80 xa2 provides peer reachability face firewalls nats possible require techniques like stun 29 upnp 30 nat pmp 31 etc xe2 x80 xa2 provides authentication s kademlia 32 participant cryptographically proves identity peer speaking avoid man inthe middle attacks xe2 x80 xa2 provides complete privacy cases bandwidth measurement section 4 17 client storage node able communicate risk eavesdroppers protocol ensure communications private default additionally framework requires way look peer network addresses unique identifier given peer xe2 x80 x99s unique identifier peer connect responsibility similar internet xe2 x80 x99s standard domain system dns 33 mapping identifier ephemeral connection address unlike dns centralized registration process achieve network overlay chord 34 pastry 35 kademlia 8 built chosen peer peer communication protocol section 4 6 implementation details 3 4 redundancy assume moment storage node offline permanently redundancy strategy store data way provides access data high x0c20 chapter 3 framework probability given number individual nodes offline state achieve specific level durability defined probability data remains available face failures products space use simple replication unfortunately ties durability network expansion factor storage overhead reliably storing data significantly increases total cost relative stored data example suppose certain desired level durability requires replication strategy makes copies data yields expansion factor 8x 800% data needs stored network bandwidth process replication results bandwidth usage fixed data discussed protocol design constraints section 2 7 blake et al 9 high bandwidth usage prevents scaling undesirable strategy ensuring high degree file durability alternative simple replication erasure codes provide efficient method achieve redundancy erasure codes established use distributed peer peer storage systems 36 xe2 x80 x9342 erasure codes encoding scheme manipulating data durability tying bandwidth usage found improve repair traffic significantly replication 9 importantly allow changes durability changes expansion factor erasure code described numbers k n block data encoded k n erasure code n total generated erasure shares k required recover original block data block data s bytes n erasure shares roughly s k bytes case k 1 replication erasure shares unique interestingly durability k 20 n 40 erasure code better k 10 n 20 erasure code expansion factor 2x risk spread nodes k 20 n 40 case considerations erasure codes important general framework better understand erasure codes increase durability increasing expansion factors following table shows choices k n expansion factor associated durability k 2 4 8 16 20 32 n 4 8 16 32 40 64 exp factor 2 2 2 2 2 2 p d p 10% 99 207366813274616% 99 858868985411326% 99 995462406878260% 99 999994620652776% 99 999999807694154% 99 999999999990544% x0c21 chapter 3 framework contrast replication requires significantly higher expansion factors durability following table shows durability replication scheme k 1 1 1 1 1 exp factor 1 2 3 10 16 n 1 2 3 10 16 p d p 10% 90 483741803595962% 98 247690369357827% 99 640050681691051% 99 999988857452166% 99 999999998036174% tables calculated xe2 x80 x99ll start simplifying assumption p monthly node churn rate fraction nodes offline month average mathematically time dependent processes modeled according poisson distribution assumed xce xbb events observed given unit time result model durability cumulative distribution function cdf poisson distribution mean xce xbb pn expect xce xbb pieces file lost monthly estimate durability consider cdf n xe2 x80 x93 k looking probability n xe2 x80 x93 k pieces file lost month file rebuilt cdf given p d e xe2 x80 x93 xce xbb n xe2 x80 x93k x xce xbb 0 expansion factor plays big role durability seen following table k 4 4 20 20 100 n 6 12 30 50 150 exp factor 1 5 3 1 5 2 5 1 5 p d p 10% 97 688471224736705% 99 999514117129605% 99 970766304935266% 99 999999999999548% 99 999999999973570% able tweak durability independently expansion factor erasure coding allows high durability achieved surprisingly low expansion factors limited bandwidth resource completely eliminating replication strategy erasure codes redundancy causes drastic decrease bandwidth footprint erasure coding results storage nodes getting paid high expansion factors dilute incoming funds byte storage nodes low expansion factors provided erasure coding allow direct passthrough income storage node operators x0cchapter 3 framework 3 4 1 22 erasure codes xe2 x80 x99 effect streaming erasure codes streaming contexts audio cds satellite communications 38 xe2 x80 x99s important point erasure coding general streaming design requirement required amazon s3 compatibility section 2 4 challenging erasure code chosen framework cds streaming added encoding small portions time instead attempting encode file section 4 8 details 3 4 2 erasure codes xe2 x80 x99 effect long tails erasure codes enable enormous performance benefit ability avoid waiting xe2 x80 x9clong tail xe2 x80 x9d response times 43 long tail response occurs situations needed server unreasonably slow operation time confluence unpredictable factors long tail responses named rare average rate occurrence highly variable nature probability density graph looks like xe2 x80 x9clong tail xe2 x80 x9d aggregate long tail responses big issue distributed system design mapreduce long tail responses called xe2 x80 x9cstragglers xe2 x80 x9d mapreduce executes redundant requests called xe2 x80 x9cbackup tasks xe2 x80 x9d sure specific stragglers long overall operation proceed waiting backup task mechanism disabled mapreduce basic operations 44% longer complete backup task mechanism causing duplicated work 44 erasure codes position create mapreduce like backup tasks storage 39 40 uploads file encoded higher k n ratio necessary desired durability guarantees upload pieces uploaded gain required redundancy remaining additional uploads canceled cancellation allows upload continue fast fastest nodes set instead waiting slowest nodes downloads similarly improved redundancy exists needed downloads served fastest peers eliminating wait temporarily slow offline peers outcome request satisfiable fastest nodes participating given transaction needing wait slower subset focusing operations result dependent fastest nodes random subpopulation turns potential liability highly variable performance individual actors great source strength distributed storage network providing great load balancing characteristics ability encode file greatly assists dynamic load balancing popular content network section 6 1 discussion plan address load balancing active files x0cchapter 3 framework 23 figure 3 1 outcomes upload download 3 5 metadata split object erasure codes select storage nodes store new pieces need track storage nodes selected allow users choose storage based geographic location performance characteristics available space features instead implicit node selection scheme consistent hashing like dynamo 25 use explicit node selection scheme directory based lookups 45 additionally maintain amazon s3 compatibility user able choose arbitrary key treated like path identify mapping data pieces node features imply necessity metadata storage system amazon s3 compatibility imposes tight requirements support hierarchical objects paths prefixes object key value storage arbitrarily large files arbitrarily large amounts files forth objects able stored retrieved arbitrary key addition deterministic iteration keys required allow paginated listing time object added edited removed entries metadata storage system need adjusted result heavy churn metadata system entire userbase metadata end sizable example suppose years network stores total exabyte data average object size 50mb erasure code selected n 40 exabyte 50mb objects 20 billion objects metadata system need track 40 nodes selected object metadata element roughly x0cchapter 3 framework 24 40 xc2 xb7 64 192 bytes info selected node plus path general overhead 55 terabytes metadata track fortunately metadata heavily partitioned user user storing 100 terabytes 50 megabyte objects incur metadata overhead 5 5 gigabytes xe2 x80 x99s worth pointing numbers vary heavily object size larger average object size metadata overhead additional framework focus enabling component xe2 x80 x94metadata storage xe2 x80 x94to interchangeable specifically expect platform incorporate multiple implementations metadata storage users allowed choose greatly assists design goal coordination avoidance users section 2 10 aside scale requirements implement amazon s3 compatibility desired api straightforward simple store metadata given path retrieve metadata given path list paginated deterministic listing existing paths delete remove path figure 2 1 details 3 6 encryption regardless storage system design constraints require total security privacy data metadata encrypted data encrypted early possible data storage pipeline ideally data leaves source computer means amazon s3 compatible interface appropriate similar client library run colocated computer user xe2 x80 x99s application encryption use pluggable mechanism allows users choose desired encryption scheme store metadata encryption scheme allow users recover data appropriate decryption mechanism cases encryption choices changed upgraded support rich access management features encryption key file having access file result access decryption keys files instead file encrypted unique key allow users share access certain selected files giving encryption details file encrypted differently different keys potentially different algorithms metadata encryption stored manner secure reliable metadata metadata file including path stored previously discussed metadata storage system encrypted deterministic hierarchical encryption scheme hierarchical encryption scheme based bip32 46 allow subtrees shared sharing parents allow files shared sharing files section 4 11 discussion path based hierarchical deterministic encryption scheme x0cchapter 3 framework 3 7 25 audits reputation incentivizing storage nodes accurately store data paramount importance viability system essential able validate verify storage nodes accurately storing asked store storage systems use probabilistic file audits called proofs retrievability way determining repair files 47 48 extending probabilistic nature common file proofs retrievability range possible files stored specific node audits case probabilistic challenges confirm high degree certainty low overhead storage node wellbehaved keeping data claims susceptible hardware failure malintent audits function xe2 x80 x9cspot checks xe2 x80 x9d 49 help calculate future usefulness given storage node storage system audits simply mechanism determine node xe2 x80 x99s degree stability failed audits result storage node marked bad result redistributing data new nodes avoiding node altogether future storage node uptime overall health primary metrics determine files need repair case proofs retrievability 47 48 auditing mechanism audit bytes files leave room false positives verifier believes storage node retains intact data actually modified partially deleted fortunately probability false positive individual partial audit easily calculable section 7 2 applied iteratively storage node detection missing altered data certain known modifiable error threshold reputation system needed persist history audit outcomes given node identities overall framework flexible requirements use system section 4 15 discussion initial approach 3 8 data repair data loss present risk distributed storage system potential causes file loss storage node churn storage nodes joining leaving network largest leading risk significant degree compared causes discussed section 2 5 network session time real world systems range hours mere minutes 7 ways data lost corruption malicious behavior bad hardware software error user initiated space reclamation issues node churn expect node churn dominant cause data loss network audits validating conforming nodes store data correctly x0cchapter 3 framework 26 mains detect storage node stops storing data correctly goes offline repair data new nodes repair data recover original data erasure code reconstruction remaining pieces regenerate missing pieces store network new storage nodes vital system incentivize storage node participants remain online longer hours encourage behavior payment strategy involve rewarding storage node operators nodes participating months years time 3 9 payments payments value attribution billing decentralized networks critical maintaining healthy ecosystem supply demand course decentralized payment systems infancy number ways framework achieve low latency high throughput transactional dependencies blockchain section 2 10 means adequately performant storage system afford wait blockchain operations operations measured milliseconds waiting cluster nodes probabilistically come agreement shared global ledger non starter framework instead emphasizes game theoretic models ensure participants network properly incentivized remain network behave rationally paid decisions modeled real world financial relationships payments transferred background settlement process behaved participants network cooperate storage nodes framework limit exposure untrusted payers confidence gained payers likely pay services rendered addition framework tracks aggregates value consumption services data stored network charging usage framework able support end end economics storage marketplace ecosystem storj network payment agnostic protocol require specific payment type network assumes ethereum based storj token default mechanism payment intend storj token primary form payment future alternate payment types implemented including bitcoin ether credit debit card ach transfer physical transfer live goats x0c4 concrete implementation believe framework xe2 x80 x99ve described relatively fundamental given design constraints framework remains freedom choosing implement component section lay initial implementation strategy expect details contained section change gradually time believe details outlined viable support working implementation framework capable providing highly secure performant durable production grade cloud storage previous version 37 publish changes concrete architecture storj improvement proposal process 50 4 1 definitions following defined terms description concrete implementation follows 4 1 1 actors client user application upload download data network peer class cohesive collection network services responsibilities different peer classes represent services network storage nodes satellites uplinks storage node peer class participates node discovery system stores data gets paid storage bandwidth uplink peer class represents application service implements libuplink wants store retrieve data peer class expected remain online like classes relatively lightweight peer class performs encryption erasure encoding coordinates peer classes behalf customer client libuplink library provides necessary functions interact storage nodes satellites directly library available number different programming languages gateway service provides compatibility layer object storage services amazon s3 libuplink exposing amazon s3 compatible api uplink cli command line interface uploading downloading files network managing permissions sharing managing accounts satellite peer class participates node discovery system caches node address information stores object metadata maintains storage node reputation aggre x0cchapter 4 concrete implementation 28 gates billing data pays storage nodes performs audits repair manages authorization user accounts users accounts trust specific satellites user run satellite expect users elect avoid operational complexity create account satellite hosted trusted party storj labs friend group workplace figure 4 1 different peer classes 4 1 2 data bucket bucket unbounded named collection files identified paths file unique path bucket path path unique identifier file bucket path arbitrary string bytes paths contain forward slashes access control boundaries forward slashes referred path separator separate path components example path videos carlsagan gloriousdawn mp4 path components videos carlsagan gloriousdawn mp4 requested encrypt paths leave customer xe2 x80 x99s application xe2 x80 x99s computer file object file object main data type system file referred path contains arbitrary bytes minimum maximum size file represented ordered collection segments segments fixed maximum size file supports limited key value userdefined fields called extended attributes like paths data contained file encrypted leaves client computer extended attribute extended attribute user defined key value field associated file like file metadata extended attributes stored encrypted segment segment represents single array bytes 0 user configurable maximum segment size section 4 8 2 details remote segment remote segment segment erasure encoded distributed network remote segment larger metadata x0cchapter 4 concrete implementation figure 4 2 files segments stripes erasure shares pieces 29 x0cchapter 4 concrete implementation 30 quired track bookkeeping includes information ids nodes data stored inline segment inline segment segment small data represents takes space corresponding data remote segment need track nodes data cases data stored xe2 x80 x9cinline xe2 x80 x9d instead stored nodes stripe stripe subdivision segment stripe fixed bytes encryption erasure encoding boundary size erasure encoding happens stripes individually encryption happen small multiple stripes time segments encrypted remote segments erasure encode stripes stripe unit audits performed section 4 8 3 details erasure share stripe erasure encoded generates multiple pieces called erasure shares subset erasure shares needed recover original stripe erasure share index identifying erasure share e g second etc piece remote segment xe2 x80 x99s stripes erasure encoded erasure shares erasure shares remote segment index concatenated concatenated group erasure shares called piece n erasure shares erasure encoding stripe n pieces processing remote segment ith piece concatenation ith erasure shares segment xe2 x80 x99s stripes section 4 8 5 details pointer pointer data structure contains inline segment data keeps track storage nodes pieces remote segment stored file metadata 4 2 peer classes overall strategy extends previous version 37 heavily mirrors distributed storage systems google file system 26 gfs like systems 27 51 52 lustre distributed file system 28 case major actors network metadata servers object storage servers clients object storage servers hold bulk data stored system metadata servers track object metadata objects located object storage servers clients provide coherent view easy access files communicating metadata object storage servers lustre xe2 x80 x99s architecture proven high performance majority 100 fastest supercomputers use lustre high performance scalable storage 28 don xe2 x80 x99t expect achieve equal performance wide area network expect dramatically better performance architectures limitation experience performance factors overall architecture previous version different names component previously x0cchapter 4 concrete implementation 31 referred storj share refer simply storage nodes centralized single bridge instance run referred satellite libstorj library backwards compatible possible refer client software uplinks 4 3 storage node main duty storage node reliably store return data node operators individuals entities excess hard drive space want earn income renting space operators download install configure storj software locally account required 1 configure disk space satellite bandwidth allowance node discovery storage nodes advertise bandwidth hard drive space available designated storj token wallet address simplify lifecycle management ephemeral files storage nodes track optional piece xe2 x80 x9ctime live xe2 x80 x9d ttl designations pieces stored specific ttl expiry data expected deleted expiration date ttl provided data expected stored indefinitely means storage nodes database expiration times occasionally clear old data storage nodes additionally track signed bandwidth allocations section 4 17 send satellites later settlement payment requires small database ttl bandwidth allocations stored sqlite 53 database storage nodes choose satellites work work multiple satellites default behavior payment come multiple sources varying payment schedules storage nodes paid specific satellites 1 returning data requested form egress bandwidth payment 2 storing data rest storage nodes expected reliably store data sent paid assumption faithfully storing data storage nodes fail random audits removed pool lose funds held escrow cover additional costs receive limited future payments storage nodes paid initial transfer data store ingress bandwidth discourage storage nodes deleting data paid storing problem previous version 37 storage nodes paid repair egress bandwidth usage satellites opt pay normal retrieval egress bandwidth usage storage nodes paid node discovery maintenance traffic storage nodes support methods delete method piece id satellite id signature associated satellite instance bandwidth allocation section 4 17 satellite id forms namespace identical piece id different satellite id refers different piece 1 registration 1099 tax form service required section 4 21 x0cchapter 4 concrete implementation 32 operation stream bytes optional ttl store bytes subrange bytes retrieved operation operations expected work ttl expires ttl provided delete operation received whichever comes storage nodes allow administrators configure maximum allowed disk space satellite bandwidth usage rolling 30 days track remaining reject operations valid signature appropriate satellite storage node developed released open source software 4 4 node identity setup storage nodes satellites uplinks generate identity certificates use network node id node discovery routing node operate certificate authority requires public private key pair self signed certificate certificate authority xe2 x80 x99s private key ideally kept cold storage prevent key compromise xe2 x80 x99s important certificate authority private key managed good operational security key rotation certificate authority require brand new node id figure 4 3 different keys certificates compose storage node xe2 x80 x99s overall identity row represents private public key pair public key node xe2 x80 x99s certificate authority determines node id s kademlia 32 node id hash public key serve proof work joining network unlike bitcoin xe2 x80 x99s proof work 23 proof work dependent trailing zero bits find hash output means node id end number trailing zero bits usable bal x0cchapter 4 concrete implementation 33 anced kademlia 8 tree cost meant sybil attacks prohibitively expensive time consuming node revocable leaf certificate key pair signed node xe2 x80 x99s certificate authority nodes use leaf key pair communication leaf signed timestamp satellites track node leaf compromised node issue new leaf later timestamp interested peers note newly seen leaf timestamps reject connections nodes older leaf certificates optimized special case peers need note leaf certificate certificate authority share timestamp 4 5 peer peer communication initially grpc 54 protocol transport layer security protocol tls 55 xc2 xb5tp 56 transport protocol added session traversal utilities nat stun functionality 29 stun provides nat traversal xc2 xb5tp provides reliable ordered delivery like tcp ledbat 57 functionality tls provides privacy authentication grpc provides multiplexing convenient programmer interface ledbat allows competing internet traffic priority providing graceful user experience home operators network usage interference time replace tls flexible secure transport framework noise protocol framework 58 reduce round trips connection handshakes situations data encrypted forward secrecy isn xe2 x80 x99t necessary authenticated communication tls noise peer ascertain id node speaking validating certificate chain hashing peer xe2 x80 x99s certificate authority xe2 x80 x99s public key estimated work went constructing node id considering number trailing zero bits end id satellites configure minimum proof work required pass audit section 4 13 time network require greater proofs work natural user intervention cases node achieve successful connection nat firewall stun 29 upnp 30 natpmp 31 similar techniques manual intervention port forwarding required future nodes unable create connection firewalls rely traffic proxying available nodes fee nodes provide assistance nodes initial stun setup public address validation forth 4 6 node discovery point storage nodes means identify communicate know address account fact storage nodes x0cchapter 4 concrete implementation 34 consumer internet connections routers constantly changing ip addresses node discovery system xe2 x80 x99s goal provide means look node xe2 x80 x99s latest address node id somewhat similar role dns provides public internet kademlia distributed hash table dht key value store built node lookup protocol utilize kademlia primary source truth dns like functionality node lookup ignoring key value storage aspects kademlia kademlia node lookup eliminates need functionality kademlia require owner based key republishing neighbor based key republishing storage retrieval values forth furthermore avoid number known attacks s kademlia 32 extensions appropriate unfortunately dhts kademlia require multiple network round trips operations makes difficult achieve millisecond level response times solve problem add basic decentralized caching service kademlia caching service live independently satellite attempt talk storage node network ongoing basis hour caching service cache known good address node evict nodes hasn xe2 x80 x99t talked certain period time storage nodes need extended know caching services expect scale reasonable future ping operations inexpensive admit new solution ultimately necessary fortunately space requirements negligible instance caching addresses network 80k nodes 5mb memory 2 based design satellite xe2 x80 x99s cache expected primary source truth results cache stale redundant storage strategy storage network resilient expected degree node churn staleness system robust lookups cache fail return incorrect addresses furthermore peer peer communication system provides peer authentication node discovery cache returns faulty deliberately misleading address lookup responses cause loss performance correctness satellite caches primary source truth repair section 4 14 requires rapid determination node online offline lookups system stop cache lookup attempt lookup kademlia cases failed audit requests fallback nonconcurrent lookup kademlia performed correct potentially stale cache information addition included satellite plan host help set known community run node discovery caches caches perform duty quickly returning address information given node id node online recently 2 assuming ordered memory list 4 tuples node id 32 bytes ip address 16 bytes ipv6 port 2 bytes timestamp 4 bytes 80000 xc2 xb7 32b 16b 2b 4b xe2 x89 x88 4 12mb x0cchapter 4 concrete implementation 35 kademlia messages use peer peer communication protocol section 4 5 includes confidentiality peer identification requires cryptographic setup connections kademlia neighbors frequent contacts cached possible kademlia message shared network nodes include available disk space satellite bandwidth availability storj wallet address metadata network needs node discovery cache collect information provided nodes allowing faster lookups 4 6 1 mitigating sybil attacks xe2 x80 x99ve adopted proof work scheme s kademlia proposes partially address sybil attacks extend kademlia application specific integration defend network given storage nodes b storage node b allowed enter storage node xe2 x80 x99s routing table storage node b present signed message satellite c storage node trusts claiming b passed audits c trusts sections 4 13 4 15 ensures nodes verified disk space opportunity participate routing layer node allowed enter routing tables considered vetted lookups progress vetted nodes sure unvetted nodes found vetted nodes unbounded lists unvetted neighbors provided xor distance unvetted neighbors farther farthest k closest vetted neighbors unvetted nodes k nearest vetted nodes date 4 7 redundancy use reed solomon erasure code 59 implement solution reducing effects long tails section 3 4 2 choose 4 numbers object store k m o n k xe2 x89 xa4 m xe2 x89 xa4 o xe2 x89 xa4 n standard reed solomon numbers k n k minimum required number pieces reconstruction n total number pieces generated creation minimum safe optimal values respectively m o value m chosen satellite notices available pieces fallen m triggers repair immediately attempt sure maintain k pieces m called r0 giroire et al 36 achieve long tail performance improvements 39 40 43 44 value o chosen uploads repairs soon o pieces finished uploading remaining pieces n canceled furthermore o chosen storing o pieces needed achieve desired durability goals n chosen storing n pieces excess durability x0cchapter 4 concrete implementation 36 figure 4 4 relationship k m o n long tail uploads tolerate n xe2 x80 x93 o slow nodes immune nodes temporarily offline time triggering repair o xe2 x80 x93 m safety buffer avoid losing data time recognize data requires repair actual repair executed m xe2 x80 x93 k section 7 3 select reed solomon numbers section 4 14 discussion repair data durability drops time 4 8 structured file storage 4 8 1 files extended attributes applications benefit able metadata alongside files amazon s3 supports xe2 x80 x9cobject metadata xe2 x80 x9d 60 assist need functionality called xe2 x80 x9cextended attributes xe2 x80 x9d posix compatible systems continue system file include limited set user specified key value pairs extended attributes stored alongside metadata file 4 8 2 files segments previous version 37 term shard referred pieces storage nodes sharding referred segmenting file smaller chunks easier processing addition erasure coding previous version terms somewhat confusing decided distinguish meaning new words x0cchapter 4 concrete implementation 37 sharding process called segmenting highest level subdivision file xe2 x80 x99s stream data called segment unfortunately general inconsistency terms literature gfs refers segments chunks 26 lustre refers segments stripes 28 use term stripes subdelineation file small consists segment segment smaller metadata required store network data stored inline metadata 3 inline segment larger files data broken large remote segments segmenting manner offers numerous advantages security privacy performance availability distributed storage systems 26 xe2 x80 x9328 51 52 segmenting large files e g videos distributing segments network reduces impact content delivery given node bandwidth demands distributed evenly network previous version 37 standardized sizes help frustrate attempts determine content given segment help obscure flow data network addition end user advantage parallel transfer similar bittorrent 62 peer peer networks lastly capping size segments allows uniform storage node filling node needs space store segment participate network client doesn xe2 x80 x99t need find nodes space large file 4 8 3 segments stripes situations xe2 x80 x99s important access subsection larger piece data file formats video files disk images support seeking subset data needed read operations creators audio cds discovered xe2 x80 x99s useful able decode small parts segment support operations 38 purpose stripe defines subset segment couple kilobytes size encryption happens small multiple stripes erasure encoding happens single stripe time use authenticated encryption encryption batch slight overhead slightly larger encryption sizes preferred audits happen stripes want audit bandwidth usage small 3 linux file system ext4 performs optimization inline inodes 61 x0cchapter 4 concrete implementation 38 reader familiar zfec library filefec mode zfec refers stripe chunk 42 4 8 4 stripes erasure shares discussed sections 3 4 4 7 erasure codes chance control network durability face unreliable storage nodes stripes boundary perform erasure encoding k n erasure code scheme n erasure shares generated stripe 59 example stripe broken 40 erasure shares n 40 20 k 20 needed reconstruct stripe 40 erasure shares 1 20th size original stripe erasure encoding single stripe time allows read small portions large segment retrieving entire segment 38 allows stream data network staging enables number useful features section 7 3 3 breakdown varying erasure code parameters affects availability redundancy 4 8 5 erasure shares pieces stripes small erasure shares smaller metadata track separately immense relative size n x0cchapter 4 concrete implementation 39 erasure shares defined index associated specifically fixed stripe given n ith share erasure code zfec library xe2 x80 x99s filefec mode 42 instead keeping track erasure shares separately pack erasure shares index piece k n scheme n pieces piece ordered concatenation erasure shares index result erasure share 1 kth stripe piece 1 kth segment k pieces needed recover segment piece store storage node satellites generate brand new randomly chosen root piece id time new upload begins uplink root piece id secret send node specific piece id storage node formed taking hash based message authentication code hmac root piece id node xe2 x80 x99s id serves obscure pieces belong storage nodes root piece id stored pointer storage nodes namespace pieces satellite id piece id satellite reused satellite satellite safely assume shared piece id refers different piece satellite different content lifecycle 4 8 6 pointers data owner need knowledge remote segment broken network pieces located recover contained pointer data structure pointer includes nodes storing pieces encryption information erasure coding details repair threshold determines redundancy segment lose triggering repair pieces stored consider repair successful details segment inline segment pointer contains entire segment xe2 x80 x99s binary data instead nodes store pieces previous version 37 data structures track aforementioned kinds information frames pointers version combined data structures single data structure elected new combined data structure pointer 4 9 metadata metadata storage system storj network predominantly stores pointers individual components storj network communicate pointer database store retrieve pointers path perform actions trivial implementation metadata storage functionality require x0cchapter 4 concrete implementation 40 simply user use preferred trusted database mongodb mariadb couchbase postgresql sqlite 53 cassandra 63 spanner 64 cockroachdb cases acceptable specific users provided users managing appropriate backups metadata types users petabytes data store likely manage reliable backups single relational database storing metadata hand downsides letting clients manage metadata traditional database system xe2 x80 xa2 availability availability user xe2 x80 x99s data tied entirely availability metadata server counterpoint availability arbitrarily good existing trusted distributed solutions cassandra spanner cockroachdb assuming appropriate effort maintaining operations furthermore individual metadata service downtime affect rest network fact network xe2 x80 xa2 durability metadata server suffers catastrophic failure backups user xe2 x80 x99s data lost true encryption keys traditional database solution considerably increases risk area encryption keys fortunately metadata periodically backed storj network turn allows track metadata metadata decreasing critical information stored xe2 x80 xa2 trust user trust metadata server hand upsides xe2 x80 xa2 control user complete control data organizational single point failure user free choose metadata store trade offs prefer run like mastodon 65 solution decentralized furthermore catastrophic scenario design worse technologies techniques application developers frequently use databases xe2 x80 xa2 simplicity projects spent multiple years shaky implementations byzantine fault tolerant consensus metadata storage expected performance complexity trade offs appendix useful product market work considerable advantage xe2 x80 xa2 coordination avoidance users need coordinate users satellite user high throughput demands set satellite avoid coordination overhead user allowing satellite operators select database allow user choose satellite weaker consistency semantics highly available transactions 15 reduce coordination overhead satellite increase performance launch goal allow customers store metadata database choosing expect look forward new systems improvements specifically component framework x0cchapter 4 concrete implementation 41 appendix xe2 x80 x99ve chosen currently avoid trying solve problem byzantine distributed consensus section 6 2 discussion future plans 4 10 satellite collection services hold metadata called satellite users network accounts specific satellite instance store file metadata manage authorization data track storage node reliability repair maintain data redundancy reduced issue payments storage nodes user xe2 x80 x99s behalf notably specific satellite instance necessarily constitute server satellite run collection servers backed horizontally scalable trusted database higher uptime storj implements thin client model delegates trust managing files xe2 x80 x99 location metadata satellite service manages data ownership uplinks able support widest possible array client applications satellites require high uptime potentially significant infrastructure especially active set files like storage nodes satellite service developed released open source software individual organization run satellite facilitate network access satellite core complex straightforward components initial release fulfills framework notwithstanding future frameworkconforming releases initial satellite standard application server wraps trusted database postgresql cassandra whichever solution metadata system chooses section 4 9 users sign specific satellite account credentials data available satellite instance available satellite instance levels export import planned section 6 2 respect customer data satellite given data unencrypted hold encryption keys knowledge object satellite able share parties existence rough size metadata access patterns system protects client xe2 x80 x99s privacy gives client complete control access data delegating responsibility keeping files available network satellite clients use satellites run party satellites store data access keys large improvement traditional data center model features satellites provide like storage node selection reputation leverage considerable network effects reputation data sets grow useful increase size indicating strong economic incentives share infrastructure information satellite providers choose operate public satellites service application developers x0cchapter 4 concrete implementation 42 delegate trust location data network specific satellite traditional object store lesser degree future updates allow distributions responsibilities levels trust customer applications satellites satellite instance components xe2 x80 xa2 xe2 x80 xa2 xe2 x80 xa2 xe2 x80 xa2 xe2 x80 xa2 xe2 x80 xa2 node discovery cache section 4 6 object metadata database indexed encrypted path section 4 9 account management authorization system section 4 12 storage node reputation statistics auditing system section 4 13 data repair service section 4 14 storage node payment service section 4 16 launch goal satellites step ahead previous system xe2 x80 x99s bridge implementation 37 point decentralization journey expect continue find ways decentralize components 4 11 encryption encryption choice authenticated encryption support aes gcm cipher salsa20 poly1305 combination nacl calls xe2 x80 x9csecretbox xe2 x80 x9d 66 authenticated encryption user know tampered data data encrypted blocks small batches stripes recommended 4kb 67 encryption key encryption batch segment segments different encryption keys nonce encryption batch monotonically incrementing previous batch entire segment nonce wraps 0 counter reaches maximum representable nonce prevent reordering attacks starting nonce segment deterministically chosen based segment number multiple segments uploaded parallel case amazon s3 xe2 x80 x99s multipart upload feature starting nonce segment calculated starting nonce file segment number scheme protects content data storage node housing data data owner retains complete control encryption key access data paths encrypted like bip32 46 encryption hierarchical deterministic path component encrypted separately explain start scheme determining secret value path component let xe2 x80 x99s given path p unencrypted path components p1 p2 pn want determine encrypted path e path components e1 e2 en assume predetermined root secret s0 root secret chosen user like encryption secrets leaves client computer recursively define si hmac si xe2 x80 x931 pi key k si x0cchapter 4 concrete implementation 43 deterministically generated si define encrypted path component ei enc k si xe2 x80 x931 pi new path e e1 e2 en hmac sha256 hmacsha512 key derivations construction allows client share access subtree path access parents paths depth example suppose client like share access paths prefix p1 p2 p3 client client client e1 e2 e3 s3 allows client decrypt access arbitrary e4 k s3 known allowing client decrypt e3 earlier generally case client decrypt access arbitrary ei 3 path encryption enabled default optional encrypted paths efficient sorted path listing challenging path encryption use perbucket feature objects sorted encrypted path deterministic relatively unhelpful client application interested sorted unencrypted paths reason users opt path encryption path encryption disabled unencrypted paths revealed user xe2 x80 x99s chosen satellite storage nodes storage nodes continue information path metadata pieces store 4 12 authorization encryption protects privacy data allowing identification tampering authorization allows prevention tampering disallowing clients making unauthorized edits users authorized able add remove edit files users authorized abilities metadata operations authorized users authenticate satellite allow access operations according authorization configuration initial metadata authorization scheme uses macaroons 68 macaroons type bearer token authorizes bearer restricted resources macaroons especially interesting allow rich contextual decentralized delegation words provide property add restrictions way restrictions later removed coordination central party use macaroons restrict operations applied encrypted paths applied way macaroons provide mechanism restrict delegated access specific encrypted path prefixes specific files specific operations read access append access account root macaroon operations validated supplied macaroon xe2 x80 x99s set caveats macaroons caveated optional expirations revocation tokens allow users revoke macaroons programmatically want restrict satellite operations satellites access en x0cchapter 4 concrete implementation 44 crypted paths authorization scheme work encrypted paths access delegation specific path prefixes path separation boundaries path components remain encryption implies reduced functionality performance path delimiters forward slash uplink authorized satellite satellite approve sign operations storage nodes including bandwidth allocations section 4 17 uplink retrieve valid signatures satellite prior operations storage nodes operations storage node require specific satellite id associated signature storage node reject operations signed appropriate satellite id storage nodes allow operations signed satellite apply objects owned explicitly granted owning satellite initial implementation detect attempt mitigate unexpected file removal rollback misbehaving satellite trust model expects user xe2 x80 x99s satellite behaved stores repairs data reliably satellite trusted unlikely repair data client xe2 x80 x99s behalf future implementation add thorough detection satellite based file system tampering scheme systems sundr sirius plutus 69 xe2 x80 x9371 4 13 audits network untrusted nodes validating nodes returning data accurately behaving expected vital ensuring properly functioning system audits way confirm nodes data claim auditors satellites send challenge storage node expect valid response challenge request storage node order prove expected data distributed storage systems including previous version storj 37 discuss merkle tree proofs audit challenges expected responses generated time storage form proof retrievability 47 merkle tree 72 metadata needed store challenges responses negligible proofs retrievability broadly classified limited unlimited schemes 49 merkle tree variety previous version limited scheme unfortunately scheme challenges expected responses pregenerated learned previous version periodic regeneration challenges storage node begin pass audits storing requested data keeping track challenges exist saving expected responses previous version began consider reed solomon erasure coding help solve problem assumption storage system storage nodes behave rationally incentives aligned data stored faithfully long assumption holds reed solomon able detect errors correct mechanisms x0cchapter 4 concrete implementation 45 berlekamp welch error correction algorithm 39 73 reed solomon erasure coding 59 small ranges stripes discussed hail system 41 use erasure coding read single stripe time challenge validate erasure share responses allows run arbitrary audits pre generated challenges perform audit choose stripe request stripe xe2 x80 x99s erasure shares storage nodes responsible run berlekamp welch algorithm 39 73 erasure shares storage nodes return correct information faulty missing responses easily identified given specific storage node audit reveal offline incorrect case node offline audit failure address node discovery cache stale fresh kademlia lookup attempted node appears offline satellite places node containment mode mode satellite calculate save expected response continue try audit node node responds successfully actively fails audit disqualified offline long node responds successfully leaves containment mode audit failures stored saved reputation system audits additionally serve opportunity test storage node latency throughput responsiveness uptime data saved reputation system important storage node frequent set random audits gain statistical power behaved storage node operating discussed section 3 7 requirement audits performed byte file additionally important byte stored system equal probability checked future audit byte system section 7 2 discussion audits required confident data stored correctly 4 14 data repair storage nodes offline xe2 x80 x94taking pieces xe2 x80 x94it necessary missing pieces rebuilt segment xe2 x80 x99s pieces fall predetermined threshold m node goes offline satellite mark nodes xe2 x80 x99 file pieces missing node discovery system xe2 x80 x99s caches reasonably accurate date information storage nodes online recently storage node changes state recently online offline trigger lookup reverse index user xe2 x80 x99s metadata database identifying segment pointers stored node segment drops appropriate minimum safety threshold m segment downloaded reconstructed missing pieces x0cchapter 4 concrete implementation 46 generated uploaded new nodes finally pointer updated include new information users choose desired durability satellite impact price considerations desired durability statistics ongoing audits directly inform reed solomon erasure code choices new repaired files thresholds set uploads successful repair needed sections 3 4 7 3 calculate values given user inputs direct implication design satellite constantly stay running user xe2 x80 x99s satellite stops running repairs stop data eventually disappear network node churn similar design value storing republishing works kademlia 8 requires owner stay online ingress inbound bandwidth demands audit repair system large given standard configuration egress outbound demands relatively small large data comes system audits repairs missing pieces sent repair audit system run bandwidth usage asymmetry means hosting providers offer free ingress especially attractive hosting location users system 4 14 1 piece hashes data repair ongoing costly operation use significant bandwidth memory processing power impacting single operator result repair resource usage aggressively minimized possible repairing segment effective minimizing bandwidth usage pieces needed reconstruction downloaded unfortunately reed solomon insufficient correcting errors redundant pieces provided instead piece hashes provide better way confident xe2 x80 x99re repairing data correctly solve problem hashes piece stored alongside piece storage node validation hash set hashes correct stored pointer repair hashes piece retrieved validated correctness pointer allowing piece validated entirety allows repair system correctly assess repair completed successfully extra redundancy task x0cchapter 4 concrete implementation 4 15 47 storage node reputation reputation metrics decentralized networks critical enabling cooperation nodes progress challenging reputation metrics ensure bad actors network eliminated participants improving security reliability durability storage node reputation divided subsystems subsystem proof work identity system second subsystem initial vetting process subsystem filtering system finally fourth system preference system goal system require short proof storage node operator invested time stake resources initially proof work mentioned section 4 3 storage nodes require proof work identity generation helps network avoid sybil attacks 74 glossed proof work difficulty set let satellite operators set satellite minimum difficulty required new data storage storage node identity generated lower difficulty satellite xe2 x80 x99s configured minimum storage node candidate new data expect satellite operators naturally increase minimum proof work difficulty requirements time reasonable balance found case changing difficulty configuration satellites leave existing data existing nodes possible investment proof schemes possible form proof stake proposed previous work 75 second subsystem slowly allows nodes join network storage node joins network reliability unknown result placed vetting process data known propose following way gather data new nodes compromising integrity network time file uploaded satellite select small number additional unvetted storage nodes include list target nodes reed solomon parameters chosen unvetted storage nodes affect durability file allow network test node small fraction data sure node reliable storage node successfully stored data long period payment period satellite start including storage node standard selection process general uploads node signed message claiming vetting process completed storage node enter nodes xe2 x80 x99 routing tables section 4 6 1 importantly storage nodes paid vetting period don xe2 x80 x99t receive data filtering system subsystem blocks bad storage nodes participating addition simply having sufficient proof work certain actions storage node disqualifying events reputation system filter nodes future uploads regardless node vetting process actions disqualifying include failing audits failing return data reasonable speed failing uptime checks x0cchapter 4 concrete implementation 48 storage node disqualified node longer selected future data storage data node stores moved new storage nodes likewise client attempts download piece storage node node stored node fails return node disqualified importantly storage nodes allowed reject fail operations penalty nodes allowed choose satellite operators work data store xe2 x80 x99s worth reiterating failing uptime checks disqualifying event storage nodes taken maintenance storage node offline adverse impact network node offline audit specific audit retried node responds successfully disqualified prevent nodes selectively failing respond audits storage node disqualified node entire vetting process node decides start brand new identity node restart vetting process beginning addition generating new node id proof work system strongly disincentivizes storage nodes cavalier reputation subsystem preference system disqualified storage nodes filtered remaining statistics collected audits establish preference better storage nodes uploads statistics include performance characteristics throughput latency history reliability uptime geographic location desirable qualities combined load balancing selection process uploads sent qualified nodes higher likelihood uploads preferred nodes non zero chance qualified node initially xe2 x80 x99ll load balancing preferences randomized scheme power choices 76 selects options entirely random chooses qualified storj network preferential storage node reputation select new data stored repair upload new files unlike disqualifying events storage node xe2 x80 x99s preferential reputation decreases file pieces moved repaired nodes process planned system storage nodes contest reputation scores best interest storage nodes good uptime pass audits return data storage nodes don xe2 x80 x99t things useful network storage nodes treated satellites unfairly accept future data satellites section 4 21 quality control plan ensure satellites incentivized treat storage nodes fairly initially storage node reputation individually determined satellite node disqualified satellite store data satellites reputation initially shared satellites time reputation determined globally x0cchapter 4 concrete implementation 4 16 49 payments storj network payments clients store data platform satellite utilize satellites pays storage nodes storage bandwidth provide network payments clients mechanism storj credit card invoice etc payments storage nodes ethereum based erc20 77 storj token previous distributed systems handled payments hard coded contracts example previous storj network utilized 90 day contracts maintain data network period time file deleted distributed storage platforms use 15 day renewable contracts delete data user login 15 days use 30 day contracts believe common use case indefinite storage best solve use case network longer use contracts manage payments file storage durations default assumption data indefinitely satellites pay storage nodes data store piece downloads storage nodes paid initial transfer data paid storing data month month end payment period satellite calculate earnings storage nodes provided storage node hasn xe2 x80 x99t disqualified storage node paid satellite data stored course month satellite xe2 x80 x99s records satellites strong incentive prefer long lived storage nodes storage node churn high satellites escrow portion storage node xe2 x80 x99s payment storage node maintained good participation uptime minimum time order greater half year storage node leaves network prematurely satellite reclaim escrowed payments storage node misses delete command node offline storing data satellite credits storage nodes paid storing file pieces eventually cleaned garbage collection process section 4 19 means storage nodes maintain higher availability maximize profits deleting files request minimizes garbage data store satellite maintains database file pieces responsible storage nodes believes storing pieces day satellite adds day xe2 x80 x99s worth accounting storage node file piece storing satellites track utilized bandwidth section 4 17 end month satellite adds bandwidth storage payments storage node earned makes payments appropriate storage nodes satellites earn revenue account holders executing audits repairing segments storing metadata satellites charge segment byte cost addition charging access retrieval segment charges cover cost pointer x0cchapter 4 concrete implementation 50 metadata byte charges cover cost data maintenance network day satellite execute number audits storage nodes network satellite charge completing audits repairs segments fall piece threshold needed repair detected storage node acts maliciously store files properly maintain sufficient availability paid services rendered funds allocated instead repair missing file pieces pay new storage nodes storing data reduce transaction fees overhead possible payments recipient initiated worth minimum value certain satellites elect use portion storage nodes xe2 x80 x99 payout cover transaction fees satellite reputation section section 4 18 details storage nodes know trust satellites 4 17 bandwidth allocation core component system requires knowing bandwidth peers previous version 37 78 exchange reports gather information transpired peers end operation peers send reports central collection service settlement peers mutually agreed straightforward determine bandwidth disagreed resorted data analysis regression determine peer greater propensity dishonesty effort catch xe2 x80 x9ccheaters xe2 x80 x9d rational nodes new version want cheating impossible protocol level solve problem turn neuman xe2 x80 x99s proxy based authorization accounting distributed systems 79 accounting protocol correctly measures resource usage delegated decentralized way neuman xe2 x80 x99s accounting protocol account holder funds cover operation account server create signed digital check transfer account holder protocol refers check proxy refer bandwidth allocation check contains information identifying account server payer payee maximum resources available operation check number prevent double spending problems 80 expiration date case account server satellite payer uplink payee storage node resource question bandwidth satellite create bandwidth allocation uplink authorized request beginning storage operation uplink transfer bandwidth allocation storage node x0cchapter 4 concrete implementation 51 storage node validate satellite xe2 x80 x99s signature perform requested operation allowed bandwidth limit storing later sending bandwidth allocation satellite payment xe2 x80 x99re inspired filecoin xe2 x80 x99s chain retrieval market small amounts data transferred time 81 instead allowing storage node cheat save bandwidth allocation performing requested operation break operation smaller requests storage node uplink stop participating protocol prematurely peer class exposed loss similar optimistic gradual release fair exchange protocol 80 support neuman xe2 x80 x99s accounting protocol little satellite overhead use restricted bandwidth allocations referred restricted proxies 79 neuman xe2 x80 x99s restricted proxies work like macaroons 68 caveats added way xe2 x80 x99t removed limiting capabilities proxy proxies use public private key cryptography means validate proxy instead original issuer uplink key pair identity section 4 4 use existing key pair instead creating new key pair restriction restricted bandwidth allocations case restricted uplink limit bandwidth allocation xe2 x80 x99s value transferred far way storage node largest bandwidth allocation received point uplink send bandwidth allocations slightly larger received storage node incentive largest allocation share xe2 x80 x9ccheck number xe2 x80 x9d cashed case operation assume satellite signed bandwidth allocation allows x bytes total uplink start sending restricted allocation small y bytes kilobytes storage node verify uplink xe2 x80 x99s authorization allocation signed correctly storage node transfer listed restricted allocation y bytes awaiting allocation uplink send allocation y larger continuing send allocations data y grown x value transaction storage node sends previously unsent data storage node sends x bytes total seen figure 4 6 pipeline requests avoid pipeline stall performance penalties request terminated time planned unexpectedly storage node largest restricted bandwidth allocation received largest restricted bandwidth allocation signed confirmation uplink uplink agreed bandwidth usage y bytes satellite xe2 x80 x99s confirmation uplink xe2 x80 x99s bandwidth allowance x storage node periodically send largest restricted bandwidth allocations received appropriate satellites point satellites pay storage node bandwidth uplink xe2 x80 x99t afford bandwidth usage satellite sign bandwidth allocation protecting satellite xe2 x80 x99s reputation likewise uplink tries use x0cchapter 4 concrete implementation figure 4 5 diagram operation figure 4 6 diagram operation 52 x0cchapter 4 concrete implementation 53 bandwidth allocated storage node decline request storage node paid maximum client agreed valid bandwidth allocations return payment don xe2 x80 x99t measure peer peer traffic bandwidth traffic measurement system tracks bandwidth storage operations storage retrievals pieces apply node discovery traffic kademlia dht generic maintenance overhead 4 18 satellite reputation satellite storj network stellar payment demand generation performance history strong incentive storage nodes avoid accepting data new satellite joins network participating storage nodes commence vetting process process limits exposure new unknown satellite building trust time highlight satellites best payment record storage nodes able configure maximum data store untrusted satellite build historical data satellite trusted future storage node operators retain manual control satellites trust won xe2 x80 x99t trust desired storage node operators elect automatically trust storj labs provided collection recommended satellites adhere strict set quality controls payment service level agreements slas protect storage node operators satellite operator wants included xe2 x80 x9ctardigrade xe2 x80 x9d approved list satellite operator required adhere set operating payment pricing parameters sign business arrangement storj labs section 4 21 details 4 19 garbage collection clients replace delete data satellites clients behalf satellites notify storage nodes longer required store data configurations delete messages issued client metadata system satellite satellite reputation line require proof deletes issued configurable minimum number storage nodes means time data deleted storage nodes online reachable receive notifications right away storage nodes temporarily unavailable miss delete messages cases unneeded data considered garbage satellites pay data expect stored storage nodes lots garbage earn x0cchapter 4 concrete implementation 54 garbage collection system employed reason introduce garbage collection free space storage nodes garbage collection algorithm method freeing longer resources precise garbage collector collects garbage exactly leaves additional garbage conservative garbage collector hand leave small proportion garbage given trade offs aim improving performance long conservative garbage collector system payment storage owed storage node high amortize cost storing garbage nodes miss initial delete messages release start conservative garbage collection strategy anticipate precise strategy near future periodically storage nodes request data structure detect differences simplest form hash stored keys allows efficient detection outof sync state detecting sync state collection use structure bloom filter 82 find data deleted returning data structure tailored node periodic schedule satellite storage node ability clean garbage data configurable tolerance satellites reject overly frequent requests data structures 4 20 uplink uplink term use identify software service invokes libuplink order interact satellites storage nodes comes forms libuplink libuplink library provides access storing retrieving data storj network gateways gateways act compatibility layers service application storj network run service co located data generated communicate directly storage nodes avoid central bandwidth costs gateway simple service layer libuplink gateway amazon s3 gateway provides s3 compatible drop interface users applications need store data don xe2 x80 x99t want bother complexities distributed storage directly uplink cli uplink cli command line application invokes libuplink allowing user upload download files create remove buckets manage file permissions related tasks aims provide experience familiar expect linux unix tools scp rsync like storage nodes satellites uplink software forms developed released open source software x0cchapter 4 concrete implementation 4 21 55 quality control branding storj network major product focuses serve distinct target markets focal points 1 creating storage supply network recruiting storage node operators 2 creating demand cloud storage paying users storj differentiate focuses experience design market segment separating supply business demand brands storj tardigrade supply market served storj brand retain storj io place learning contribute extra storage bandwidth storj network includes storage node setup documentation frequently asked questions faqs tutorials users brands able access source code community storj io demand business served tardigrade brand directed tardigrade io experience focused partners customers purchased decentralized storage bandwidth network expectation high durability resilience reliability backed industry leading service level agreement sla includes offers free trials satellite selection documentation faqs tutorials forth xe2 x80 x9ctardigrade xe2 x80 x9d brand additionally serve satellite quality credentialing system set satellite storj io satellite listed official tardigrade satellite considered xe2 x80 x9ctardigrade quality xe2 x80 x9d benefit directly storj labs xe2 x80 x99 demand generation activities operator pass certain compliance quality requirements quality controls continuously audit rank satellites behavior durability compliance performance addition satellite operator adhere particular business policies pricing storage node recruitment slas storage node payments forth satellite operators tardigrade network business relationship storj labs defines things franchise fees revenue sharing entities storj labs assume responsibilities including demand generation brand enforcement satellite operator support end user support united states form 1099 tax filing compliance 4 insurance maintenance overall network quality compliance quality controls implemented ensure storage nodes paid fairly satellites able continuously meet slas tardigrade products 4 form 1099 required law payments individual given year exceeding total $600 x0c5 walkthroughs following collection common use case examples different types transactions data system 5 1 upload user wants upload file user begins transferring data instance uplink xe2 x80 xa2 uplink chooses encryption key starting nonce segment begins encrypting incoming data authenticated encryption flows network xe2 x80 xa2 uplink buffers data knows incoming segment short inline segment remote segment inline segments small stored satellite rest walkthrough assume remote segment remote segments involve technology stack xe2 x80 xa2 uplink sends request satellite prepare storage segment request object contains api credentials macaroons identity certificates receiving request satellite xe2 x80 xa2 confirm uplink appropriate authorization funds request uplink account specific satellite xe2 x80 xa2 selection nodes adequate resources conform bucket xe2 x80 x99s configured durability performance geographic reputation requirements xe2 x80 xa2 return list nodes contact information unrestricted bandwidth allocations chosen root piece id uplink information begin parallel connections chosen storage nodes measuring bandwidth section 4 17 xe2 x80 xa2 uplink begin breaking segment stripes erasure encode stripe xe2 x80 xa2 generated erasure shares concatenated pieces transfer storage node parallel xe2 x80 xa2 erasure encoding configured encode pieces needed eliminate long tail effect lead significant improvement visible performance allowing uplink cancel slowest uploads x0cchapter 5 walkthroughs 57 xe2 x80 xa2 data continue transfer maximum segment size hit stream ends whichever sooner xe2 x80 xa2 hashes piece written end piece stream storage node store largest restricted bandwidth allocation ttl segment exists data data identified storage node specific piece id delegating satellite id upload aborted reason storage node largest restricted bandwidth allocation received client uplink behalf satellite throw away relevant request data assuming success xe2 x80 xa2 uplink encrypts random encryption key chose file utilizing deterministic hierarchical key xe2 x80 xa2 uplink upload pointer object satellite contains following information xe2 x80 x93 storage nodes ultimately successful xe2 x80 x93 encrypted path chosen segment xe2 x80 x93 erasure code algorithm xe2 x80 x93 chosen piece id xe2 x80 x93 encrypted encryption key metadata xe2 x80 x93 hash piece hashes xe2 x80 x93 signature finally uplink proceed segment continuing process segments entire stream completed segment gets new encryption key segment xe2 x80 x99s starting nonce monotonically increases previous segment segment stored stream contain additional metadata xe2 x80 xa2 xe2 x80 xa2 xe2 x80 xa2 xe2 x80 xa2 segments stream contained large segments bytes starting nonce segment extended attributes metadata periodically storage nodes later send largest restricted bandwidth allocation received upload appropriate satellite payment upload happens amazon s3 multipart upload interface uploaded segment individually x0cchapter 5 walkthroughs 5 2 58 download user wants download file user sends request data uplink uplink tries reduce number round trips satellite speculatively requesting pointers segments addition pointer segment uplink needs segment pointer learn size object size number segments decrypt data future release uplink tell satellite byte ranges needed satellite respond appropriate segment pointers segment pointer requested satellite xe2 x80 xa2 validate uplink access download segment pointer funds pay download xe2 x80 xa2 generate unrestricted bandwidth allocation piece makes segment xe2 x80 xa2 look contact information storage nodes listed pointer xe2 x80 xa2 return requested segment pointer bandwidth allocations node contact info piece uplink determine segments necessary data request received request remaining segment pointers needed xe2 x80 xa2 necessary segment pointers returned requested segments inline uplink initiate parallel requests measuring bandwidth section 4 17 appropriate storage nodes appropriate erasure share ranges inside stored piece xe2 x80 xa2 erasure shares necessary recovery long tails eliminated significant visible performance improvement gained allowing uplink cancel slowest downloads xe2 x80 xa2 uplink combine retrieved erasure shares stripes decrypt data download aborted reason storage node largest restricted bandwidth allocation received throw away relevant request data way storage nodes later send largest restricted bandwidth allocation received download appropriate satellite later payment 5 3 delete user wants delete file delete operation received uplink uplink requests segment pointers file segment pointer satellite x0cchapter 5 walkthroughs 59 xe2 x80 xa2 validate uplink access delete segment pointer xe2 x80 xa2 generate signed agreement deletion segment storage node knows satellite expecting delete proceed xe2 x80 xa2 look contact information storage nodes listed pointer xe2 x80 xa2 return segments agreements contact information remote segments uplink initiate parallel requests appropriate storage nodes signal pieces removed xe2 x80 xa2 storage nodes return signed message indicating storage node received delete operation delete file bookkeeping information removed xe2 x80 xa2 uplink upload signed messages received working storage nodes satellite satellite require adjustable percent total storage nodes successfully sign messages ensure uplink notifying storage nodes object deleted xe2 x80 xa2 satellite remove segment pointers stop charging customer stop paying storage nodes xe2 x80 xa2 uplink return success status periodically storage nodes ask satellite generated garbage collection messages update storage nodes offline main deletion event satellites reject requests garbage collection messages happen frequently section 4 19 details 5 4 user wants file uplink receives request moving file new path uplink requests segment pointers file segment pointer satellite xe2 x80 xa2 validates uplink access download xe2 x80 xa2 returns requested segment metadata segment pointer uplink xe2 x80 xa2 decrypts metadata encryption key derived path xe2 x80 xa2 calculates path new destination xe2 x80 xa2 encrypts metadata new encryption key derived new path uplink requests satellite add modified segment pointers remove old segment pointers atomic compare swap operation satellite validate x0cchapter 5 walkthroughs 60 xe2 x80 xa2 uplink appropriate authorization remove old path create new path xe2 x80 xa2 content old path hasn xe2 x80 x99t changed overall operation started validation successful satellite perform operation storage node receive request related file complexity atomic pointer batch modifications efficient operations implemented release network 5 5 copy user wants copy file uplink receives request copying file new path uplink requests segment pointers file segment pointer satellite xe2 x80 xa2 validates uplink access download xe2 x80 xa2 looks contact information storage nodes listed pointer xe2 x80 xa2 returns requested segment metadata new root piece id contact information segment pointer uplink xe2 x80 xa2 decrypts metadata encryption key derived path xe2 x80 xa2 changes path new destination xe2 x80 xa2 invokes copy operation storage nodes pointer duplicate piece new piece id xe2 x80 xa2 waits storage nodes respond duplicated data removes nodes unsuccessful xe2 x80 xa2 encrypts metadata new piece id new encryption key derived new path finally uplink uploads modified segment pointers satellite importantly okay storage nodes de duplicate storage store actual copy data matters storage node identify data old new piece id piece ids receives delete operation piece id continue working pieces deleted node free space 5 6 list user wants list files x0cchapter 5 walkthroughs 61 xe2 x80 xa2 request listing page objects received uplink xe2 x80 xa2 uplink translate request unencrypted paths encrypted paths xe2 x80 xa2 uplink request satellite appropriate page encrypted paths xe2 x80 xa2 satellite validate uplink appropriate access return requested list page xe2 x80 xa2 finally uplink decrypt results return 5 7 audit satellite queue segment stripes audited set storage nodes queue filled mechanisms xe2 x80 xa2 mechanism satellite populates queue periodically selecting segments randomly stripes segments random segments maximum size sufficiently approximates goal choosing byte audit uniformly random xe2 x80 xa2 second mechanism satellite chooses stripe audit identifying storage nodes fewer recent audits storage nodes satellite select stripe random data contained storage node satellites work process queue report errors xe2 x80 xa2 stripe request satellite perform entire download operation small stripe range filtering nodes containment mode unlike standard downloads stripe request need performant satellite attempt download erasure shares stripe wait slow storage nodes xe2 x80 xa2 receiving shares possible generous timeout erasure shares analyzed discover wrong satellites note storage nodes return invalid data storage node returns invalid data satellite disqualify storage node future exchanges case disqualification satellite pay storage node going forward select storage node new data xe2 x80 xa2 storage nodes respond cryptographic checksum expected audit result created stored placing unresponsive nodes containment containment node continue given audit unresponsive passes disqualified 5 8 data repair repair process parts detects unhealthy files second repairs detection straightforward x0cchapter 5 walkthroughs 62 xe2 x80 xa2 satellite periodically ping storage node knows audit process standard node discovery ping operations xe2 x80 xa2 satellite track nodes fail respond mark xe2 x80 xa2 node marked marked bad audit process pointers point storage node considered repair pointers track minimum allowable redundancy pointer stored good online storage nodes added repair queue worker process segment pointers repair queue segment pointer taken repair queue xe2 x80 xa2 worker download pieces reconstruct entire segment piece hashes stored pieces section 4 14 1 unlike audits pieces accurate repair needed unlike streaming downloads repair system wait entire segment starting xe2 x80 xa2 piece hashes validated signature pointer downloaded pieces validated validated piece hashes incorrect pieces thrown away count source failed audits xe2 x80 xa2 correct pieces recovered missing pieces regenerated xe2 x80 xa2 satellite selects new nodes uploads new pieces new nodes normal upload process xe2 x80 xa2 satellite updates pointer xe2 x80 x99s metadata 5 9 payment payment process works follows xe2 x80 xa2 satellite choose rollup period period time xe2 x80 x94defaulting day xe2 x80 x94that payment data rest calculated purely period chosen accounting actual payments happen frequent schedule xe2 x80 xa2 roll period satellite consider files believes currently stored storage node satellites track payments owed storage node rollup period based data kept storage node xe2 x80 xa2 finally storage nodes periodically xe2 x80 x94defaulting monthly xe2 x80 x94send bandwidth allocation reports satellite receives bandwidth allocation report calculates owed funds outstanding data rest calculations sends funds storage node xe2 x80 x99s requested wallet address x0c6 future work storj work progress features planned future versions chapter discuss especially interesting areas want consider improvements concrete implementation 6 1 hot files content delivery occasionally users system end delivering files popular anticipated storage node operators welcome opportunity paid bandwidth usage data demand popular files outstrip available bandwidth capacity form dynamic scaling needed fortunately satellites authorize accesses pieces meter rate limit access popular files file xe2 x80 x99s demand starts grow current resources serve satellite opportunity temporarily pause accesses necessary increase redundancy file storage nodes continue allowing access reed solomon erasure coding useful property assume k n encoding k pieces needed n total non negative integer number x n pieces k n x encoding exact pieces k n encoding means redundancy easily scaled little overhead practical example suppose file encoded k 20 n 40 scheme satellite discovers needs double bandwidth resources meet demand satellite download 20 pieces 40 generate 40 pieces new k 20 n 80 scheme store new pieces 40 new nodes xe2 x80 x94without changing data original 40 nodes xe2 x80 x94store file k 20 n 80 scheme 20 80 pieces needed allows requests adequately load balance 80 pieces demand outstrips supply 20 pieces needed generate redundancy manner satellite temporarily increase redundancy 20 250 requests load balanced 250 nodes piece 250 unique 20 pieces required regenerate original file hand satellite need pay storage nodes increased redundancy content delivery manner increased rest costs high demand addition bandwidth costs hand content delivery desired highly geographically redundant scheme provides naturally x0cchapter 6 future work 6 2 64 improving user experience metadata initial concrete implementation place significant burdens satellite operator maintain good service level high availability high durability regular payments regular backups expect large degree variation quality satellites led implement quality control program section 4 21 time clients satellites want reduce dependence satellite operators enjoy efficient data portability satellites downloading uploading data manually plan spend significant time improving user experience number ways short term plan build metadata import export system users backups metadata transfer metadata satellites medium term plan reduce size exports considerably backup process automatic seamless possible expect build system periodically major portion metadata directly network long term plan architect satellite platform hope eliminate satellite control metadata entirely viable byzantine fault tolerant consensus algorithm arise biggest challenge finding right balance coordination avoidance byzantine fault tolerant consensus storage nodes interact share encoded pieces files operating performance levels users expect platform competing traditional cloud storage providers team continue research viable means achieve end section 2 10 appendix discussions aren xe2 x80 x99t tackling byzantine fault tolerant consensus problem right away x0c7 selected calculations 7 1 object repair costs fundamental challenge system choose system parameters expansion factor repair bandwidth minimum provide acceptable level durability fortunately wondering good prior research problem xe2 x80 x9cpeer peer storage systems practical guideline lazy xe2 x80 x9d 36 excellent guide work follows conclusions end result mathematical framework determines network durability repair bandwidth given reed solomon parameters average node lifetime reconstruction rate following summary results explanation implications variable mttf xce xb1 mrt xce xb3 d n k m lr 1 lr ed br bwr description mean time failure 1 mttf mean reconstruction time 1 mrt total bytes network total number pieces segment rs encoding pieces needed rebuild segment rs encoding repair threshold loss rate durability expansion factor ratio data repair bandwidth total repair bandwidth network lr 1 m m 1 ln n m k xe2 x80 x93 1 x12 x13m xe2 x80 x93k 2 xce xb1 xce xb3 ed n k xce xb1 n xe2 x80 x93 m k k ln n m bwr d xc2 xb7 br br equations demonstrate repair bandwidth impacted node churn linearly expected lower mean time node failure triggers frequent rebuilds x0cchapter 7 selected calculations 66 bandwidth usage loss rate sensitive high node churn increases exponentially xce xb1 necessitates stable nodes lifetimes months achieve acceptable network durability section 7 3 depth discussion node churn affects erasure code parameters 7 1 1 bandwidth limits usable space repair affects storage nodes xe2 x80 x99 participation bandwidth usage constrains usable disk space consider storage node 1 tb available space stated monthly bandwidth limit 500 gb xe2 x80 x99s known framework storage node expect repair 50% data given month assuming stored object served store 333 gb node causes bandwidth allowed words paid bandwidth plus repair bandwidth equal bandwidth limit higher repair rates equal lower effective storage size nodes serving paid data frequently sensitive effect practice paid bandwidth rate vary type data stored node ratios monitored closely determine appropriate usable space limits network evolves time x0c67 chapter 7 selected calculations audit false positive risk rely bayesian approach determine probability storage node maintaining stored pieces faithfully high level seek answer following question consecutive successful audits change estimate probability node continue return successful audits model audit process binomial random variable unknown probability success p xe2 x88 x88 0 1 audit independent bernoulli trial known conjugate prior binomial distribution beta distribution xce xb2 b posterior follows beta distribution 83 use mean posterior distribution bayes estimator given p x b n b parameters prior distribution x number successes observed n audits assumption audit successful arrive bayes estimate success probability p n b n jeffrey s prior 1 00 6 0 95 probability estimate 7 5 4 3 2 1 audit success probability estimate 0 90 0 85 0 80 0 75 0 0 0 2 0 4 0 6 audit success probability 0 8 1 0 0 25 50 75 100 125 150 175 number audits assumed successful 200 figure 7 1 jeffrey xe2 x80 x99s prior estimate audit success probability heavily weighted near 0 near 1 uniform prior audit success probability estimate 1 00 1 04 0 95 probability estimate 7 2 1 02 1 00 0 98 0 90 0 85 0 80 0 75 0 70 0 96 0 0 0 2 0 4 0 6 audit success probability 0 8 1 0 0 65 0 25 50 75 100 125 150 175 number audits assumed successful 200 figure 7 2 uniform prior assumption placed estimated audit success probability probabilities assumed equally likely choose prior derive numerical estimate audit success probability based number audits performed reasonable choices bayesian priors restrict attention popular choices uniform prior jeffrey xe2 x80 x99s x0c68 chapter 7 selected calculations prior 84 uniform prior xce xb2 1 1 initializes experiment assigning equal probability possible outcomes probability success drawn uniform distribution 0 1 jeffrey xe2 x80 x99s prior xce xb2 0 5 0 5 assumed probability success falls extreme node return successful audit probability near 0 probability near 1 number audits 0 20 40 80 200 audit success estimate given uniform prior 0 5 0 9545 0 9762 0 9878 0 99505 audit success estimate given jeffrey xe2 x80 x99s prior 0 5 0 9762 0 9878 0 9938 0 99751 table 7 1 estimate audit success probability number audits assumed successful find estimated probability success begins 0 5 information known node audits performed estimate quickly jumping 99% 80 audits jeffrey xe2 x80 x99s prior table 7 1 present results obtained priors remark established bayesian approach allows rapidly gain confidence node xe2 x80 x99s ability return successful audit given success probability estimate tends closer 1 consecutive audit success x0c69 chapter 7 selected calculations 7 3 choosing erasure parameters context storing erasure coded segment decentralized network consider loss piece different perspectives 7 3 1 direct piece loss direct piece loss assume specific segment erasure pieces lost according certain rate point modeling straightforward pieces lost rate 0 p 1 start n pieces piece decay follows exponential decay pattern form n 1 xe2 x80 x93 p t t time elapsed according units rate 1 account multiple checks month extend n 1 xe2 x80 x93 p m rebuild threshold controls segment rebuilt solve n 1 xe2 x80 x93 p m t taking ceiling necessary determine long n pieces segment decay m pieces works clear given parameters n m smallest t t aln m n ln 1 xe2 x80 x93p p long expect segment repairs 7 3 2 indirect piece loss modeling indirect piece loss suppose fixed rate nodes drop network month 2 holding pieces segment consideration describe probability d dropped nodes storing n pieces specific segment turn hypergeometric probability distribution suppose c nodes replaced month c total nodes network probability d nodes storing piece segment given n x01 c xe2 x80 x93n x01 p x d d cc xe2 x80 x93d x01 7 1 c mean nc c determine long number pieces fall desired threshold m iterating holding overall churn c fixed reducing number existing pieces distribution xe2 x80 x99s mean iteration counting number iterations required example iteration number existing pieces reduced nc c instead n pieces network parameter 7 1 n xe2 x80 x93 nc c pieces changing parameter mean 7 1 iteration 2 extend model considering multiple checks month direct piece loss case assuming c nodes lost 1 th month instead assuming c nodes lost month number checks month yields initial hypergeometric probability distribution mean nc ac 1 assume proportion p 0 1 pieces lost month t given months rate taken desired time interval 2 x0cchapter 7 selected calculations 70 cases single multiple segment integrity checks month track number iterations number available pieces fall repair threshold number determine expected number rebuilds month given segment 7 3 3 numerical simulations indirect piece loss produce decision tables table 7 2 showcasing worst case mean segment rebuild outcomes based simulating piece loss segments encoded varying reed solomon parameters assume k n rs encoding scheme n pieces generated k pieces needed reconstruction different values n assume segment undergoes process repair m pieces remain network different values m n initial table use simplifying assumption pieces network lost constant rate month 3 node churn data corruption problems arrive value mean rebuilds month consider single segment encoded n pieces distributed uniformly randomly nodes network simulate conditions leading rebuild uniformly randomly select subset nodes total population designate failed multiple times simulated month scaling piece loss rate linearly according number segment integrity checks month 4 nodes failed bring number pieces repair threshold m segment rebuilt track number rebuilds course 24 months repeat simulation 1000 iterations simulating 1000 year periods single segment number rebuilds 99th percentile higher number rebuilds occurring 1000 iterations words choose value value observed cumulative distribution function cdf describing number rebuilds year period 0 99 value divided number months arrive mean rebuilds month value example approach shown figure 7 3 perform experiment network 10 000 nodes observing network size directly impact mean rebuilds month value single segment working assumption constant rate loss month 5 forming decision tables consider calculations different choices k n m mean time failure affect durability repair bandwidth 3 constant rate viewed mean poisson distribution modeling piece loss month example monthly network piece loss rate assumed 0 1 network size 10% 10 segment integrity checks performed month assume average 1% pieces lost checks 5 represent piece loss proportion nodes selected uniformly randomly total network proportion scales directly network size overall number pieces lost stays networks different sizes 4 x0c71 chapter 7 selected calculations rebuilds 24 months rebuilds 24 months 1 0 cumulative observed density 1000 iterations observed density 1000 iterations 0 25 0 20 0 15 0 10 0 05 0 00 16 18 20 22 24 number rebuilds 26 0 8 0 6 0 4 0 2 0 0 16 18 20 22 24 number rebuilds 26 figure 7 3 left density number rebuilds 24 month period repeated 1000 iterations right cdf number rebuilds case mean rebuilds month value taken 26 24 xe2 x89 x88 1 083 99 7% chance segment rebuilt 26 times course 24 months looking lowest repair bandwidth meets durability requirements mttf months 1 6 12 1 6 12 1 6 12 k 20 20 20 30 30 30 40 40 40 n 40 40 40 60 70 80 80 120 120 m 35 30 25 35 40 45 60 50 45 repair bandwidth ratio 9 36 0 87 0 31 3 40 0 60 0 31 5 21 0 52 0 24 durability nines 0 9999 8 0 9999 17 0 9999 13 0 9999 4 0 9999 15 0 9999 25 0 9999 4 0 9999 14 0 9999 11 table 7 2 decision tables showing relationship churn mttf reed solomon parameters k n m repair bandwidth ratio durability 7 3 4 conclusion conclude observing models tuned target specific network scenarios requirements network require set reed solomon parameters different network require general closer m n 1 rebuilds month expected fixed churn rate having larger ratio m n increases file durability given churn rate comes expense bandwidth repairs triggered maintain low mean rebuilds month value maintaining higher file durability aim increase value n feasible given network conditions latency download speed etc allows lower relative value m jeopardizing file durability x0cchapter 7 selected calculations 72 informally takes longer lose pieces given fixed network size churn rate maximize durability minimizing repair bandwidth usage n large existing network conditions allow allows value m relatively closer k reducing mean rebuilds month value turn lowers repair bandwidth example assume network mean time failure months suppose consider file encoded different rs parameters 20 40 schema 30 80 schema set m m k 10 cases observe table bandwidth repair ratio 0 87 20 40 case 0 60 40 80 case encoding schemes similar durability repair cases triggered k 10 pieces left mean rebuilds month empirically theoretically lower 40 80 case m k 10 x0ca distributed consensus explain trying solve byzantine distributed consensus xe2 x80 x99s worth brief discussion history distributed consensus 1 non byzantine distributed consensus computerized data storage systems began necessity single computers storing retrieving data unfortunately environments system continue operating times single computer failure grind important process halt result researchers sought ways enable groups computers manage data specific computer required operation spreading ownership data multiple computers increase uptime face failures increase throughput spreading work processors forth research field long challenging fortunately led exciting technology biggest issue getting group computers agree messages lost impacts decision making succinctly described xe2 x80 x9ctwo generals xe2 x80 x99 problem xe2 x80 x9d 85 1 armies try communicate face potentially lost messages armies agreed attack shared enemy decide time armies attack time failure assured armies send messengers messengers captured enemy armies know time attack army agreed time ultimately generic solution generals xe2 x80 x99 problem finite number messages impossible engineering approaches embrace uncertainty necessity distributed systems trade offs deal uncertainty systems embrace consistency means system choose downtime inconsistent answers systems embrace availability means system chooses potentially inconsistent answers downtime widely cited cap theorem 12 13 states system choose consistency availability partition tolerance inevitability network failures partition tolerance nonnegotiable partition happens system choose sacrifice consistency availability systems sacrifice accident cap theorem consistency specifically linearizability means read receives recent write error inconsistent answer means system returned recent write obviously failing generally number consistency models acceptable making trade offs linearizability sequential consistency causal consistency pram consis1 earlier described problem groups gangsters 86 x0cchapter distributed consensus 74 tency eventual consistency read write consistency etc models discussing history events appears participants distributed system 2 amazon s3 generally provides read write consistency cases provide eventual consistency instead 89 distributed databases provide eventual consistency default dynamo 25 cassandra 63 linearizability distributed system desirable weakly consistent models useful building block higher level data structures operations distributed locks coordination techniques initially early efforts build linearizable distributed consensus centered phase commit phase commit suffered issues similar generals xe2 x80 x99 problem 1985 flp impossibility paper 90 proved algorithm reach linearizable consensus bounded time 1988 barbara liskov brian oki published viewstamped replication algorithm 91 linearizable distributed consensus algorithm unaware vr publication leslie lamport set prove linearizable distributed consensus impossible 92 instead 1989 proved possible publishing paxos algorithm 93 significantly popular wasn xe2 x80 x99t officially published journal 1998 ultimately algorithms large common despite lamport xe2 x80 x99s claims paxos simple 94 papers published challenging assertion google xe2 x80 x99s description attempts implement paxos described paxos live 95 paxos moderately complex 96 attempt try fill details protocol entire basis raft algorithm rooted trying wrangle simplify complexity paxos 24 ultimately upsetting decades reliable implementations paxos raft viewstamped replication 97 chain replication 98 zab 99 exist ongoing work improve situation 100 101 arguably google xe2 x80 x99s early success spending time build internal paxos service distributed lock system chubby 102 google xe2 x80 x99s famous early internal data storage tools bigtable 103 depend chubby correctness spanner 64 xe2 x80 x94perhaps incredible distributed databases world xe2 x80 x94is largely phase commit multiple paxos groups 2 byzantine distributed consensus mentioned design constraints expect nodes rational byzantine altruistic unfortunately previous algorithms discussed assume collection altruistic nodes reliable distributed consensus algorithms game changing applications requiring fault tolerant 2 differing consistency models new worth reading kyle kingbury xe2 x80 x99s excellent tutorial 87 xe2 x80 x99re wondering computers xe2 x80 x99t use current time order events mind exceedingly difficult computers agree 88 x0cchapter distributed consensus 75 storage success mixed byzantine fault tolerant world number attempts solve byzantine fault tolerant distributed consensus problem field exploded release bitcoin 23 early stages note particularly interested pbft 104 barbara liskov solution q u 105 fab 106 107 bitcoin zyzzyva 108 107 rbft 109 tangaroa 110 tendermint 111 aliph 112 hashgraph 113 honeybadgerbft 114 algorand 115 casper 116 tangle 117 avalanche 118 parsec 119 120 algorithms additional trade offs non byzantine distributed consensus algorithms don xe2 x80 x99t require deal potential uncooperative nodes example pbft 104 causes significant network overhead pbft client attempt talk majority participants individually reply client bitcoin 23 intentionally limits transaction rate changing proof ofwork difficulty post bitcoin protocols require participants copy change histories 3 xe2 x80 x99re avoiding byzantine distributed consensus ultimately existing solutions fall short goal minimizing coordination section 2 10 flexible paxos 101 significantly better normal paxos steadystate avoiding coordination completely unusable byzantine environment distributed ledger xe2 x80 x9ctangle like xe2 x80 x9d approaches suffer inability prune history retain significant global coordination overhead excited look forward fast scalable byzantine fault tolerant solution building blocks listed previous discussion clear arisen reducing risk avoiding problem entirely x0cb attacks distributed system variety attack vectors exist common distributed systems storage specific apply distributed storage system b 1 spartacus spartacus attacks identity hijacking possible unmodified kademlia 8 node assume identity node receive fraction messages intended node simply copying node id allows targeted attacks specific nodes data spartacus attack mitigation addressed s kademlia 32 implementing node ids public key hashes requiring messages signed spartacus attacker system unable generate corresponding private key unable sign messages participate network b 2 sybil sybil attacks 74 involve creation large amounts nodes attempt disrupt network operation hijacking dropping messages kademlia 8 vulnerable sybil attacks adoption s kademlia 32 proof work identity generation section 4 4 reduces vulnerability degree storage node reputation system involves prolonged initial vetting period nodes complete trusted significant amounts data membership kademlia routing tables vetting system discussed sections 4 6 1 4 15 prevents large influx new nodes taking incoming data existing reputable storage nodes proving longevity b 3 eclipse eclipse attack attempts isolate node set nodes network graph ensuring outbound connections reach malicious nodes eclipse attacks hard identify malicious nodes function normally cases eclipsing certain important messages information storj addresses eclipse attacks public key hashes node ids signatures based public keys multiple disjoint network lookups prescribed s kademlia 32 larger network harder prevent node finding portion network uncontrolled attacker long storage node satellite x0cchapter b attacks 77 introduced portion network controlled attacker point public key hashes signatures ensure man middle attacks impossible multiple disjoint network lookups ensure kademlia routing prohibitively expensive bias avoid eclipse attack remains sure new nodes appropriately introduced behaved node network bootstrapping process end storj labs run known verified bootstrap nodes b 4 honest geppetto attack attacker operates large number xe2 x80 x9cpuppet xe2 x80 x9d storage nodes network accumulating reputation data time certain threshold reached pulls strings puppet execute hostage attack data involved simply drops storage node network best defense attack create network sufficient scale attack ineffective meantime partially prevented relatedness analysis storage nodes bayesian inference downtime latency network route attributes assess likelihood storage nodes operated organization satellites attempt distribute pieces unrelated storage nodes possible b 5 hostage bytes hostage byte attack storage specific attack malicious storage nodes refuse transfer pieces portions pieces order extort additional payments clients reed solomon encoding ought sufficient defeat attacks sort client simply download necessary number pieces nodes multiple malicious nodes collude gain control pieces file mitigations discussed honest geppetto attack apply help avoid situation b 6 cheating storage nodes uplinks satellites measuring bandwidth signatures minimizes risk uplink storage nodes uplink interact storage node sending signed restricted allocation restriction limits risk low level storage node comply protocol expected order restricted allocations storage nodes satellites commence vetting process limits exposure storage nodes allowed decline requests untrusted satellites x0cchapter b attacks b 7 78 faithless storage nodes satellites storage nodes satellites built require authentication signatures serving download requests reasonable imagine modification storage node satellite provide downloads paying requestor network faithless satellite data privacy significantly compromised strong client encryption protects contents file inspection storj designed protect compromised clients b 8 defeated audit attacks typical merkle proof verification requires pre generated challenges responses periodic regeneration challenges storage node begin pass audits storing requested data instead request random stripe erasure shares storage nodes run berlekamp welch algorithm 73 erasure shares storage nodes return correct information faulty missing responses easily identified new storage nodes placed vetting process audits passed section 4 15 details x0cc primary user benefits designed storj network provide users better security availability performance economics xe2 x80 x94across wide variety use cases xe2 x80 x94than premise storage solutions traditional centralized cloud storage bulk paper describes design considerations overcome challenges highly decentralized system appendix describes end result significant improvement traditional approaches c 1 security designed system equivalent spreading encrypted sand encrypted beach data encrypted client reaching system data sharded distributed large number independently operated disk drives larger network independently operated storage nodes typical scenario 20 40 reed solomon setup file distributed 40 different disk drives global network 100 000 independently operated nodes previous version storj network 150 000 independently operated nodes compromise individual file bad actor locate compromise roughly 40 different drives operated different provider network 100 000 drives actor able compromise drives reconstruct file bad actor decrypt 256 bit aes encrypted data keys held end user wouldbe bad actor repeat process entirely different set potential drives file wish obtain design possible storj satellite operators storage node operators bad actors compromise end user data level decentralization network creates powerful disincentives malicious actors centralized trove data target c 2 availability centralized cloud providers employ strategies provide protection individual drive failures immune system wide events storms power outages floods earthquakes operator error design flaws network overload attacks compromise entire data centers centralized providers calculate publish theoretically high availability numbers calculations depend drive failures uncorrelated fact x0cchapter c primary user benefits 80 data center chances individual drive failing highly correlated chances drive failing decentralized system contrast node operated different individual different location separate personnel power network access forth chance individual node failing entirely uncorrelated chances drives failing result kinds availability obtain subject storms power outages xe2 x80 x9cblack swan xe2 x80 x9d events chance individual drive failing storj network higher centralized cloud chance collective failure e g losing 20 40 independent drives vanishingly small addition chance losing file correlated chances losing second file c 3 performance read intensive use cases storj network deliver superior performance taking advantage parallelism storage nodes located close xe2 x80 x9cthe edge xe2 x80 x9d reducing latency experienced recipients data physically far data center houses data read performance benefits parallelism particular erasure coding scheme use ensures slow drives slow networks networks drives experiencing temporarily high load limit throughput adjust k n ratio dramatically improve download streaming speeds imposing kinds high costs associated cdn networks c 4 economics data created world doubled year price cloud storage declined 10% year years number potential explanations supply demand public cloud storage operators large capital investments building network data centers incur significant costs power personnel security fire suppression forth pricing structure allow recoup costs structure industry inherently oligopolistic handful public cloud companies comprise largest companies market cap planet microsoft google amazon alibaba price decreases provider quickly matched providers little incentive providers drop prices gain market share decentralized network contrast little marginal cost storage node operator experience vast majority operators existing live equipment significant spare capacity additional cost storage node operator terms capital personnel running drive capacity x0cchapter c primary user benefits 81 consume significantly power running drive excess space careful management relative caps operators experience increased bandwidth costs consequently operating node represents nearly pure margin supply cost savings passed end users designed market mechanisms demand prevent satellite operator cornering market providing healthy margin farmers demand partners satellite operators believe able provide profitable storage services fraction cost equivalent centralized cloud storage providers x0cbibliography 1 identity theft resource center cyberscout annual number data breaches exposed records united states 2005 2018 millions https www statista com statistics 273550 data breaches recorded unitedstates number breaches records exposed 2018 2 knowledge sourcing intelligence llp cloud storage market forecasts 2017 2022 https www researchandmarkets com research lf8wbx cloud storage 2017 3 dan shearer eu cloud privacy crash https kopano com kopano documents eu cloud privacy pdf 2017 4 gartner inc gartner forecasts worldwide public cloud revenue grow 21 4 percent 2018 https www gartner com en newsroom press releases 2018 0412 gartner forecasts worldwide public cloud revenue grow 21 percent 2018 2018 5 synergy research group cloud growth rate increased q1 amazon maintains market share dominance https www srgresearch com articles cloudgrowth rate increased q1 amazon maintains market share dominance 2018 6 backblaze inc long hard drives 2018 hard drive stats https www backblaze com blog hard drive stats q1 2018 2018 7 sean rhea dennis geels timothy roscoe john kubiatowicz handling churn dht proceedings annual conference usenix annual technical conference atec xe2 x80 x9904 page 10 berkeley usa 2004 usenix association 8 petar maymounkov david mazi xc3 xa8res kademlia peer peer information system based xor metric revised papers international workshop peer peer systems iptps xe2 x80 x9901 pages 53 xe2 x80 x9365 london uk 2002 springer verlag 9 charles blake rodrigo rodrigues high availability scalable storage dynamic peer networks pick proceedings 9th conference hot topics operating systems volume 9 hotos xe2 x80 x9903 page 1 berkeley usa 2003 usenix association 10 comcast inc xfinity data usage center xe2 x80 x93faq https dataplan xfinity com faq 2018 11 amitanand s aiyer lorenzo alvisi allen clement mike dahlin jean philippe martin carl porth bar fault tolerance cooperative services proceedings twentieth acm symposium operating systems principles sosp xe2 x80 x9905 pages 45 xe2 x80 x9358 new york ny usa 2005 acm x0cbibliography 83 12 seth gilbert nancy lynch brewer xe2 x80 x99s conjecture feasibility consistent available partition tolerant web services sigact news 33 2 51 xe2 x80 x9359 june 2002 13 seth gilbert nancy lynch perspectives cap theorem computer 45 2 30 xe2 x80 x9336 february 2012 14 daniel abadi consistency tradeoffs modern distributed database system design cap story computer 45 2 37 xe2 x80 x9342 february 2012 15 peter bailis aaron davidson alan fekete ali ghodsi joseph m hellerstein ion stoica highly available transactions virtues limitations proc vldb endow 7 3 181 xe2 x80 x93192 november 2013 16 peter bailis alan fekete michael j franklin ali ghodsi joseph m hellerstein ion stoica coordination avoidance database systems proc vldb endow 8 3 185 xe2 x80 x93196 november 2014 17 chenggang wu jose m faleiro yihan lin joseph m hellerstein anna kvs scale icde 2018 18 joseph m hellerstein declarative imperative experiences conjectures distributed logic sigmod rec 39 1 5 xe2 x80 x9319 september 2010 19 peter alvaro neil conway joseph m hellerstein william r marczak consistency analysis bloom calm collected approach cidr 2011 20 kyle kingsbury consistency models clickable map https jepsen io consistency 2018 21 paolo viotti marko vukoli xc4 x87 consistency non transactional distributed storage systems acm comput surv 49 1 19 1 xe2 x80 x9319 34 june 2016 22 joseph hellerstein anna crazy fast super scalable flexibly consistent kvs https rise cs berkeley edu blog anna kvs 2018 23 satoshi nakamoto bitcoin peer peer electronic cash system https bitcoin org bitcoin pdf 2008 24 diego ongaro john ousterhout search understandable consensus algorithm proceedings 2014 usenix conference usenix annual technical conference usenix atc xe2 x80 x9914 pages 305 xe2 x80 x93320 berkeley usa 2014 usenix association 25 giuseppe decandia deniz hastorun madan jampani gunavardhan kakulapati avinash lakshman alex pilchin swaminathan sivasubramanian peter vosshall werner vogels dynamo amazon xe2 x80 x99s highly available key value store proceedings acm sigops symposium operating systems principles sosp xe2 x80 x9907 pages 205 xe2 x80 x93220 new york ny usa 2007 acm 26 sanjay ghemawat howard gobioff shun tak leung google file system proceedings nineteenth acm symposium operating systems principles sosp xe2 x80 x9903 pages 29 xe2 x80 x9343 new york ny usa 2003 acm x0cbibliography 84 27 konstantin shvachko hairong kuang sanjay radia robert chansler hadoop distributed file system proceedings 2010 ieee 26th symposium mass storage systems technologies msst msst xe2 x80 x9910 pages 1 xe2 x80 x9310 washington dc usa 2010 ieee computer society 28 lustre introduction lustre architecture http wiki lustre org images 6 64 lustrearchitecture v4 pdf 2017 29 j rosenberg r mahy p matthews d wing session traversal utilities nat stun rfc 5389 rfc editor october 2008 http www rfc editor org rfc rfc5389 txt 30 iso iso iec 29341 1 2011 information technology xe2 x80 x94 upnp device architecture 2011 31 s cheshire m krochmal nat port mapping protocol nat pmp rfc 6886 rfc editor april 2013 http www rfc editor org rfc rfc6886 txt 32 ingmar baumgart sebastian mies s kademlia practicable approach secure key based routing icpads pages 1 xe2 x80 x938 ieee computer society 2007 33 p mockapetris domain names implementation specification std 13 rfc editor november 1987 http www rfc editor org rfc rfc1035 txt 34 ion stoica robert morris david karger m frans kaashoek hari balakrishnan chord scalable peer peer lookup service internet applications proceedings 2001 conference applications technologies architectures protocols computer communications sigcomm xe2 x80 x9901 pages 149 xe2 x80 x93160 new york ny usa 2001 acm 35 antony t rowstron peter druschel pastry scalable decentralized object location routing large scale peer peer systems proceedings ifip acm international conference distributed systems platforms heidelberg middleware xe2 x80 x9901 pages 329 xe2 x80 x93350 london uk 2001 springer verlag 36 fr xc3 xa9d xc3 xa9ric giroire julian monteiro st xc3 xa9phane p xc3 xa9rennes peer peer storage systems practical guideline lazy ieee global communications conference globecom 12 2010 37 shawn wilkinson tome boshevski josh brandoff james prestwich gordon hall patrick gerbes philip hutchins chris pollard storj peer peer cloud storage network v2 0 https storj io storjv2 pdf 2016 38 vijay k bhargava stephen b wicker ieee communications society ieee information theory society reed solomon codes applications edited stephen b wicker vijay k bhargava ieee communications society ieee information theory society co sponsors ieee press piscataway nj 1994 39 jeff wendling jt olds introduction reed solomon https innovation vivint com introduction reed solomon bc264d0794f8 2017 x0cbibliography 85 40 netanel raviv yuval cassuto rami cohen moshe schwartz erasure correction scalar codes presence stragglers corr abs 1802 02265 2018 41 kevin d bowers ari juels alina oprea hail high availability integrity layer cloud storage proceedings 16th acm conference computer communications security ccs xe2 x80 x9909 pages 187 xe2 x80 x93198 new york ny usa 2009 acm 42 zooko wilcox zfec filefec py xe2 x80 x99s encode file https github com tahoe lafs zfec commit 2594d395923dd945cd62 2007 43 jeffrey dean luiz andr xc3 xa9 barroso tail scale communications acm 56 74 xe2 x80 x9380 2013 44 jeffrey dean sanjay ghemawat mapreduce simplified data processing large clusters commun acm 51 1 107 xe2 x80 x93113 january 2008 45 j paiva l rodrigues policies efficient data replication p2p systems 2013 international conference parallel distributed systems pages 404 xe2 x80 x93411 dec 2013 46 peter wuille bip32 hierarchical deterministic wallets https github com bitcoin bips blob master bip 0032 mediawiki 2012 47 ari juels burton s kaliski jr pors proofs retrievability large files proceedings 14th acm conference computer communications security ccs xe2 x80 x9907 pages 584 xe2 x80 x93597 new york ny usa 2007 acm 48 hovav shacham brent waters compact proofs retrievability proceedings 14th international conference theory application cryptology information security advances cryptology asiacrypt xe2 x80 x9908 pages 90 xe2 x80 x93107 berlin heidelberg 2008 springer verlag 49 kevin d bowers ari juels alina oprea proofs retrievability theory implementation proceedings 2009 acm workshop cloud computing security ccsw xe2 x80 x9909 pages 43 xe2 x80 x9354 new york ny usa 2009 acm 50 shawn wilkinson sip01 sip purpose guidelines 2016 https github com storj sips blob master sip 0001 md 51 michael ovsiannikov silvius rus damian reeves paul sutter sriram rao jim kelly quantcast file system proc vldb endow 6 11 1092 xe2 x80 x931101 august 2013 52 salman niazi mahmoud ismail seif haridi jim dowling steffen grohsschmiedt mikael ronstr xc3 xb6m hopsfs scaling hierarchical file system metadata newsql databases proceedings 15th usenix conference file storage technologies fast xe2 x80 x9917 pages 89 xe2 x80 x93103 berkeley usa 2017 usenix association 53 d richard hipp et al sqlite https www sqlite org 2000 x0cbibliography 86 54 google inc grpc https grpc io docs guides index html accessed 2018 55 t dierks e rescorla transport layer security tls protocol version 1 2 rfc 5246 rfc editor august 2008 http www rfc editor org rfc rfc5246 txt 56 arvid norberg utorrent transport protocol http www bittorrent org beps bep 0029 html 2009 57 s shalunov g hazel j iyengar m kuehlewind low extra delay background transport ledbat rfc 6817 rfc editor december 2012 http www rfc editor org rfc rfc6817 txt 58 trevor perrin noise protocol framework https noiseprotocol org noise pdf 2018 59 irving s reed gustave solomon polynomial codes certain finite fields journal society industrial applied mathematics 8 2 300 xe2 x80 x93304 1960 60 amazon inc amazon simple storage service object metadata https docs aws amazon com amazons3 latest dev usingmetadata html accessed 2018 61 tao ma ext4 add inline data support https lwn net articles 468678 2011 62 bram cohen bittorrent protocol specification http www bittorrent org beps bep 0003 html 2008 63 avinash lakshman prashant malik cassandra decentralized structured storage system sigops oper syst rev 44 2 35 xe2 x80 x9340 april 2010 64 james c corbett jeffrey dean michael epstein andrew fikes christopher frost jj furman sanjay ghemawat andrey gubarev christopher heiser peter hochschild wilson hsieh sebastian kanthak eugene kogan hongyi li alexander lloyd sergey melnik david mwaura david nagle sean quinlan rajesh rao lindsay rolig dale woodford yasushi saito christopher taylor michal szymaniak ruth wang spanner google xe2 x80 x99s globally distributed database osdi 2012 65 eugen rochko mastodon self hosted globally interconnected microblogging community https github com tootsuite mastodon 2016 66 daniel j bernstein cryptography nacl https cr yp highspeed naclcrypto 20090310 pdf 2009 67 daniel j bernstein nacl validation verification https nacl cr yp valid html 2016 68 arnar birgisson joe gibbs politz xc3 x9alfar erlingsson ankur taly michael vrable mark lentczner macaroons cookies contextual caveats decentralized authorization cloud network distributed system security symposium 2014 x0cbibliography 87 69 jinyuan li maxwell krohn david mazi xc3 xa8res dennis shasha secure untrusted data repository sundr proceedings 6th conference symposium opearting systems design implementation volume 6 osdi xe2 x80 x9904 pages 9 xe2 x80 x939 berkeley usa 2004 usenix association 70 eu jin goh hovav shacham nagendra modadugu dan boneh sirius securing remote untrusted storage ndss volume 3 pages 131 xe2 x80 x93145 2003 71 mahesh kallahalla erik riedel ram swaminathan qian wang kevin fu plutus scalable secure file sharing untrusted storage proceedings 2nd usenix conference file storage technologies fast xe2 x80 x9903 pages 29 xe2 x80 x9342 berkeley usa 2003 usenix association 72 ralph c merkle digital signature based conventional encryption function carl pomerance editor advances cryptology xe2 x80 x94 crypto xe2 x80 x9987 pages 369 xe2 x80 x93378 berlin heidelberg 1988 springer 73 lloyd r welch elwyn r berlekamp error correction algebraic block codes patent us4633470a 1986 74 john r douceur sybil attack revised papers international workshop peer peer systems iptps xe2 x80 x9901 pages 251 xe2 x80 x93260 london uk 2002 springer verlag 75 shawn wilkinson james prestwich sip02 bounding sybil attacks identity cost 2016 https github com storj sips blob master sip 0002 md 76 michael mitzenmacher power choices randomized load balancing ieee trans parallel distrib syst 12 10 1094 xe2 x80 x931104 october 2001 77 fabian vogelsteller vitalik buterin erc 20 token standard 2015 https github com ethereum eips blob master eips eip 20 md 78 braydon fuller sip09 bandwidth reputation accounting 2017 https github com storj sips blob master sip 0009 md 79 b c neuman proxy based authorization accounting distributed systems 13th international conference distributed computing systems pages 283 xe2 x80 x93291 1993 80 bruce schneier applied cryptography 2nd ed protocols algorithms source code c john wiley sons inc new york ny usa 1995 81 protocol labs filecoin decentralized storage network https filecoin io filecoin pdf 2017 82 burton h bloom space time trade offs hash coding allowable errors commun acm 13 7 422 xe2 x80 x93426 july 1970 83 asit p basu david w gaylor james j chen estimating probability occurrence tumor rare cancer zero occurrence sample regulatory toxicology pharmacology 23 2 139 xe2 x80 x93 144 1996 x0cbibliography 88 84 harold jeffreys invariant form prior probability estimation problems proceedings royal society london series mathematical physical sciences 186 1007 453 xe2 x80 x93461 1946 85 jim gray notes data base operating systems operating systems advanced course pages 393 xe2 x80 x93481 london uk 1978 springer verlag 86 e akkoyunlu k ekanadham r v huber constraints tradeoffs design network communications proceedings fifth acm symposium operating systems principles sosp xe2 x80 x9975 pages 67 xe2 x80 x9374 new york ny usa 1975 acm 87 kyle kingsbury strong consistency models https aphyr com posts 313 strong consistency models 2014 88 justin sheehy queue 13 3 20 20 xe2 x80 x9320 27 march 2015 89 amazon inc amazon simple storage service data consistency model https docs aws amazon com amazons3 latest dev introduction html consistencymodel accessed 2018 90 michael j fischer nancy lynch michael s paterson impossibility distributed consensus faulty process j acm 32 2 374 xe2 x80 x93382 april 1985 91 brian m oki barbara h liskov viewstamped replication new primary copy method support highly available distributed systems proceedings seventh annual acm symposium principles distributed computing podc xe2 x80 x9988 pages 8 xe2 x80 x9317 new york ny usa 1988 acm 92 leslie lamport time parliament website note https www microsoft com en research publication time parliament accessed 2018 93 leslie lamport time parliament acm trans comput syst 16 2 133 xe2 x80 x93169 1998 94 leslie lamport paxos simple https www microsoft com en research publication paxos simple 2001 95 tushar deepak chandra robert griesemer joshua redstone paxos live engineering perspective 2006 invited talk proceedings 26th annual acm symposium principles distributed computing 2007 96 robbert van renesse deniz altinbuken paxos moderately complex acm comput surv 47 3 42 1 xe2 x80 x9342 36 february 2015 97 barbara liskov james cowling viewstamped replication revisited technical report mit csail tr 2012 021 mit july 2012 98 robbert van renesse fred b schneider chain replication supporting high throughput availability proceedings 6th conference symposium operating systems design implementation volume 6 osdi xe2 x80 x9904 page 7 berkeley usa 2004 usenix association x0cbibliography 89 99 flavio paiva junqueira benjamin c reed marco serafini zab high performance broadcast primary backup systems ieee ifip 41st international conference dependable systems networks dsn pages 245 xe2 x80 x93256 2011 100 iulian moraru david g andersen michael kaminsky consensus egalitarian parliaments proceedings fourth acm symposium operating systems principles sosp xe2 x80 x9913 pages 358 xe2 x80 x93372 new york ny usa 2013 acm 101 h howard d malkhi spiegelman flexible paxos quorum intersection revisited arxiv e prints august 2016 102 mike burrows chubby lock service loosely coupled distributed systems proceedings 7th symposium operating systems design implementation osdi xe2 x80 x9906 pages 335 xe2 x80 x93350 berkeley usa 2006 usenix association 103 fay chang jeffrey dean sanjay ghemawat wilson c hsieh deborah wallach mike burrows tushar chandra andrew fikes robert e gruber bigtable distributed storage system structured data 7th usenix symposium operating systems design implementation osdi pages 205 xe2 x80 x93218 2006 104 miguel castro barbara liskov practical byzantine fault tolerance proceedings symposium operating systems design implementation osdi xe2 x80 x9999 pages 173 xe2 x80 x93186 berkeley usa 1999 usenix association 105 michael abd el malek gregory r ganger garth r goodson michael k reiter jay j wylie fault scalable byzantine fault tolerant services proceedings twentieth acm symposium operating systems principles sosp xe2 x80 x9905 pages 59 xe2 x80 x9374 new york ny usa 2005 acm 106 jean philippe martin lorenzo alvisi fast byzantine consensus ieee trans dependable secur comput 3 3 202 xe2 x80 x93215 july 2006 107 abraham g gueta d malkhi l alvisi r kotla j p martin revisiting fast practical byzantine fault tolerance arxiv e prints december 2017 108 ramakrishna kotla zyzzyva speculative byzantine fault tolerance acm transactions computer systems tocs 27 issue 4 article 7 december 2009 109 p l aublin s b mokhtar v qu xc3 xa9ma rbft redundant byzantine fault tolerance 2013 ieee 33rd international conference distributed computing systems pages 297 xe2 x80 x93306 july 2013 110 christopher n copeland hongxia zhong tangaroa byzantine fault tolerant raft 2014 x0cbibliography 90 111 jae kwon tendermint consensus mining https tendermint com docs tendermint pdf 2014 112 pierre louis aublin rachid guerraoui nikola kne xc5 xbeevi xc4 x87 vivien qu xc3 xa9ma marko vukoli xc4 x87 700 bft protocols acm trans comput syst 32 4 12 1 xe2 x80 x9312 45 january 2015 113 leemon baird swirlds hashgraph consensus algorithm fair fast byzantine fault tolerance 2016 114 andrew miller yu xia kyle croman elaine shi dawn song honey badger bft protocols cryptology eprint archive report 2016 199 2016 https eprint iacr org 2016 199 115 yossi gilad rotem hemo silvio micali georgios vlachos nickolai zeldovich algorand scaling byzantine agreements cryptocurrencies proceedings 26th symposium operating systems principles sosp xe2 x80 x9917 pages 51 xe2 x80 x9368 new york ny usa 2017 acm 116 vitalik buterin virgil griffith casper friendly finality gadget corr abs 1710 09437 2017 117 serguei popov tangle https iota org iota whitepaper pdf 2018 118 team rocket snowflake avalanche novel metastable consensus protocol family cryptocurrencies https ipfs io ipfs qmuy4jh5mgnzvlkjies1rwm4yuvjh5o2fyopnpvywrrvgv 2018 119 pierre chevalier bart xc5 x82omiej kami xc5 x84ski fraser hutchison qi ma spandan sharma protocol asynchronous reliable secure efficient consensus parsec http docs maidsafe net whitepapers pdf parsec pdf 2018 120 james mickens saddest moment login logout 2013 https scholar harvard edu files mickens files thesaddestmoment pdf x0c