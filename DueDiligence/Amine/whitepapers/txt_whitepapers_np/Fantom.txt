OPERA Reasoning about continuous common
 knowledge in asynchronous distributed systems
 Sang Min Choia Jiho Parka Quan Nguyena Kiyoung Jangb Hyunjoon
 Cheonb Yo Sub Hanb Byung Ik Ahna
 aFANTOM Lab FANTOM Foundation
 bDepartment of Computer Science Yonsei University
 Abstract
 This paper introduces a new family of consensus protocols namely Lachesis 
 class denoted by L for distributed networks with guaranteed Byzantine fault
 tolerance Each Lachesis protocol L in L has complete asynchrony is lead 
 erless has no round robin no proof of work and has eventual consensus 
 The core concept of our technology is the OPERA chain generated by
 the Lachesis protocol In the most general form each node in Lachesis has a
 set of k neighbours of most preference When receiving transactions a node
 creates and shares an event block with all neighbours Each event block is
 signed by the hashes of the creating node and its k peers The OPERA
 chain of the event blocks is a Directed Acyclic Graph (DAG) it guarantees
 practical Byzantine fault tolerance (pBFT) Our framework is then presented
 using Lamport timestamps and concurrent common knowledge 
 Further we present an example of Lachesis consensus protocol L0 of our
 framework Our L0 protocol can reach consensus upon 2/3 of all partici 
 pants agreement to an event block without any additional communication
 overhead L0 protocol relies on a cost function to identify k peers and to
 generate the DAG based OPERA chain By creating a binary 
 ag table that
 stores connection information and share information between blocks Lach 
 esis achieves consensus in fewer steps than pBFT protocol for consensus 
 Keywords Consensus algorithm Byzantine fault tolerance Lachesis
 protocol Lamport timestamp root Clotho Atropos Main chain
 Preprint submitted to FANTOM TECHNICAL REPORT October 4 2018
 Contents
 1 Introduction 3
 1 1 Motivation 3
 1 2 Generic framework of L Protocols 4
 1 3 Lachesis protocol L0 7
 1 4 Contributions 8
 2 Related work 9
 2 1 Lamport timestamps 9
 2 2 Concurrent common knowledge 10
 2 3 Consensus algorithms 11
 2 4 DAG based Approaches 11
 3 Generic framework of Lachesis Protocols 14
 3 1 OPERA chain 14
 3 2 Main chain 14
 3 3 Lachesis Consensus Algorithm (LCA) 16
 3 4 Node Structure 16
 3 5 Event block creation 18
 3 6 Topological ordering of events using Lamport timestamps 18
 3 7 Topological consensus ordering 19
 3 8 Peer selection algorithm 21
 4 Lachesis Consensus Protocol L0 21
 4 1 Root Selection 22
 4 2 Clotho Selection 23
 4 3 Atropos Selection 24
 4 4 Peer selection algorithm via Cost function 27
 5 Discussions 31
 5 1 Lamport timestamps 31
 5 2 Semantics of Lachesis protocols 32
 6 Conclusion 33
 6 1 Future work 33
 1
 7 Appendix 35
 7 1 Proof of Lachesis Consensus Algorithm 35
 7 1 1 Preliminaries 35
 7 1 2 Proof of Byzantine Fault Tolerance for Lachesis Con 
 sensus Algorithm 35
 7 2 Response to Attacks 39
 7 2 1 Transaction Flooding 39
 7 2 2 Parasite chain attack 39
 7 2 3 Double Spending 39
 7 2 4 Long range attack 40
 7 2 5 Bribery attack 40
 7 2 6 Denial of Service 41
 7 2 7 Sybil 41
 8 Reference 42
 2
 1 Introduction
 Beyond the success of cryptocurrencies blockchain has recently emerged
 as a technology platform that oers secure decentralized consistent transac 
 tion ledgers and has powered innovations across domains including nancial
 systems supply chains and health care Despite the high demand in dis 
 tributed ledger technology [26] commercialization opportunities have been
 obstructed by long processing time for consensus and high power consump 
 tion These issues have been addressed in consensus algorithms such as
 [4 9 23 24] 
 Distributed database systems often address Byzantine fault tolerance [13]
 in which up to just under one third of the participant nodes may be compro 
 mised Consensus algorithms ensures the integrity of transactions between
 participants over a distributed network [13] and is equivalent to the proof
 of Byzantine fault tolerance in distributed database systems [1 12] Byzan 
 tine consensus is not guaranteed for deterministic completely asynchronous
 system with unbounded delays [8] But achieving consensus is feasible for
 nondeterministic system with probability one 
 There are several approaches to consensus in distributed system The
 original Nakamoto consensus protocol in Bitcoin uses Proof of Work (PoW) 
 which requires large amounts of computational work to generate the blocks by
 participants [19] Alternative schemes such as Proof Of Stake (PoS) [25 14]
 have been proposed PoS uses participants stakes to generate the blocks
 respectively Another approach utilizes directed acyclic graphs (DAG) [16 
 23 24 21 17] to facilitate consensus 
 Examples of DAG based consensus algorithms include Tangle [22] Byte 
 ball [5] and Hashgraph [2] Tangle selects the blocks to connect in the net 
 work utilizing accumulated weight of nonce and Monte Carlo Markov Chain
 (MCMC) Byteball generates a main chain from the DAG and reaches con 
 sensus through index information of the chain Hashgraph connects each
 block from a node to another random node Hashgraph searches whether
 2/3 members can reach each block and provides a proof of Byzantine fault
 tolerance via graph search 
 1 1 Motivation
 Practical Byzantine Fault Tolerance (pBFT) allows all nodes to success 
 fully reach an agreement for a block (information) when a Byzantine node
 exists [3] In pBFT consensus is reached once a created block is shared with
 3
 other participants and the share information is shared with others again
 [10 18] After consensus is achieved the block is added to the participants 
 chains [3 6] Currently it takes O(N4) for pBFT 
 HashGraph [2] proposes \gossip about gossip and virtual voting to reach
 consensus There are several limitations with HashGraph First the algo 
 rithm operates on a known network which needs full awareness of all author 
 itative participants Second gossip propagation is slow and latency increases
 to O(n) with n participants Third it remains unclear whether virtual voting
 is faster than chain weight aka longest chain/proof of work concept These
 issues are gossip problems and not consensus problems 
 We are interested in a new approach to address the aforementioned issues
 in pBFT approaches [3 10 18] and HashGraph [2] Specically we propose a
 new consensus algorithm that addresses the following questions (1) Can we
 reach local consensus in a k cluster faster for some k (2) Can we make gos 
 sips faster such as using a broadcast based gossip subset (3) Can continuous
 common knowledge be used for consensus decisions with high probability 
 (4) Can complex decisions be reduced to binary value consensus 
 In this paper we propose a new approach that can quickly search for
 Byzantine nodes within the block DAG In particular we introduce a new
 class of consensus protocols namely Lachesis protocol denoted by L The
 core idea of Lachesis is to use a new DAG structure the OPERA chain 
 which allows faster path search for consensus We then propose an example
 of the Lachesis protocol class which is called the Lachesis protocol L0 
 1 2 Generic framework of L Protocols
 We introduce a generic framework of Lachesis protocols called L The ba 
 sic idea of Lachesis protocol is a DAG based asynchronous non deterministic
 protocol that guarantees pBFT We propose OPERA chain | a new DAG
 structure for faster consensus Lachesis protocol generates each block asyn 
 chronously and the Lachesis algorithm achieves consensus by conrming how
 many nodes know the blocks using the OPERA chain Figure 1 shows an
 example of OPERA chain constructed through a Lachesis protocol 
 The main concepts of Lachesis are given as follows 
  Event block All nodes can create event blocks as time t The structure
 of an event block includes the signature generation time transaction
 history and hash information to references The information of the
 referenced event blocks can be copied by each node The rst event
 block of each node is called a leaf event 
 4
 1
 1â€™
 1â€™â€™â€™â€™
 1â€™â€™
 1â€™â€™â€™
 2â€™
 2
 3â€™
 4
 3
 5â€™
 5
 6
 7
 6â€™
 7â€™â€™
 7â€™
 8
 8â€™
 9
 10
 Figure 1 An Example of OPERA Chain
  Lachesis protocol Lachesis protocol is the rule set to communicate be 
 tween nodes When each node creates event blocks it determines which
 nodes choose other nodes to broadcast to Node selection can be ran 
 dom or via some cost function 
  Happened before Happened before is the relationship between nodes
 which have event blocks If there is a path from an event block x
 to y then x Happened before y \x Happened before y means that
 the node creating y knows event block x 
  Root An event block is called a root if either (1) it is the rst generated
 event block of a node or (2) it can reach more than two thirds of other
 roots Every root can be candidate for Clotho 
  Root set Root set (Rs) is the set of all roots in the frame The cardinality
 of the set is 2n=3 < Rs  n where n is the number of all nodes 
  Frame Frame f is a natural number that separates Root sets The frame
 increases by 1 in case of a root in the new set (f + 1) And all event
 blocks between the new set and the previous Root set are included in
 the frame f 
 5
  Flag table The Flag table stores reachability from an event block to an 
 other root The sum of all reachabilities namely all values in 
 ag
 table indicates the number of reacheabilities from an event block to
 other roots 
  Lamport timestamps For topological ordering Lamport timestamps al 
 gorithm uses the happened before relation to determine a partial order
 of the whole event block based on logical clocks 
  Clotho A Clotho is a root that satises that they are known by more than
 2n=3 nodes and more than 2n=3 nodes know the information that they
 are known in nodes A Clotho can be a candidate for Atropos 
  Atropos An Atropos is assigned consensus time through the Lachesis con 
 sensus algorithm and is utilized for determining the order between event
 blocks Atropos blocks form a Main chain which allows time consensus
 ordering and responses to attacks 
  Reselection To solve the byzantine agreement problem each node rese 
 lects a consensus time for a Clotho based on the collected consensus
 time in the root set of the previous frame When the consensus time
 reaches byzantine agreement a Clotho is conrmed as an Atropos and
 is then used for time consensus ordering 
  OPERA chain The OPERA chain is the local view of the DAG held by
 each node this local view is used to identify topological ordering select
 Clotho and create time consensus through Atropos selection 
  Main Chain Main chain is a core subset of the OPERA chain It is
 comprised of Atropos event blocks Thus the OPERA chain uses Main 
 chain to nd rapid ordering between event blocks In OPERA chain 
 each event block is assigned a proper consensus position 
 As a motivating example Figure 2 illustrates how consensus is reached
 through the path search in the OPERA chain In the gure leaf set denoted
 by Rs0 consists of the rst event blocks created by individual participant
 nodes V is the set of event blocks that do not belong neither in Rs0 nor in
 any root set Rsi Given a vertex v in V [ Rsi there exists a path from v
 that can reach a leaf vertex u in Rs0 Let r1 and r2 be root event blocks in
 root set Rs1 and Rs2 respectively r1 is the block where a quorum or more
 6
 
 
 
 
 r1
 r2
 r3
 Leaf set an initial set of vertices by participants
 Vertex set (Vn) a set of vertices created by participants
 Root set (Rn) blocks created by participants
 Leaf events V1 V2 V3
 Figure 2 Consensus Method through Path Search in a DAG (combines chain with con 
 sensus process of pBFT)
 blocks exist on a path that reaches a leaf event block Every path from r1 to
 a leaf vertex will contain a vertex in V1 Thus if there exists a vertex r in
 V1 such that r is created by more than a quorum of participants then r is
 already included in Rs1 Likewise r2 is a block that can be reached for Rs1
 including r1 through blocks made by a quorum of participants For all leaf
 event blocks that could be reached by r1 they are shared with more than
 quorum participants through the presence of r1 The existence of the root
 r2 shows that information of r1 is shared with more than a quorum This
 kind of a path search allows the chain to reach consensus in a similar manner
 as the pBFT consensus processes It is essential to keep track of the blocks
 satisfying the pBFT consensus process for quicker path search our OPERA
 chain and Main chain keep track of these blocks 
 1 3 Lachesis protocol L0
 We now introduce a new specic Lachesis consensus protocol called L0 
 The new protocol L0 is a DAG based asynchronous non deterministic proto 
 col that guarantees pBFT L0 generates each block asynchronously and uses
 the OPERA chain for faster consensus by checking how many nodes know
 the blocks 
 In this L0 protocol we propose several algorithms In particular we
 7
 introduce an algorithm in which a node can identify lazy participants from
 cost eective peers | say its k peers We must stress that a generic Lachesis
 protocol does not depend on any k peer selection algorithm each node can
 choose k peers randomly Each message created by a node is then signed
 by the creating node and its k peers We also introduce a 
 ag table data
 structure that stores connection information of event blocks The 
 ag table
 allows us to quickly traverse the OPERA chain to nd reachability between
 event blocks 
 OPERA chain can be used to optimize path search By using certain
 event blocks (Root Clotho and Atropos) Main chain | a core subgraph of
 OPERA chain can maintain reliable information between event blocks and
 reach consensus Generating event blocks via Lachesis protocol the OPERA
 chain and Main chain are updated frequently and can respond strongly to
 attack situations such as forking and parasite attack Further using the 
 ag
 table over the OPERA chain consensus can be quickly reached and the
 ordering between specic event block can be determined 
 1 4 Contributions
 In summary this paper makes the following contributions 
  We propose a new family L of Lachesis protocols We introduce the
 OPERA chain and Main chain for faster consensus 
  We dene a topological ordering of nodes and event blocks in the
 OPERA chain By using Lamport timestamps the ordering is more
 intuitive and reliable in distributed system We introduce a 
 ag table
 at each block to improve root detection 
  We present proof of how a DAG based protocol can implement concur 
 rent common knowledge for consistent cuts 
  The Lachesis protocols allow for faster node synchronization with k 
 neighbor broadcasts 
  A specic Lachesis protocol L0 is then introduced with specic algo 
 rithms The benets of Lachesis protocol L0 include (1) root selection
 algorithm via 
 ag table (2) an algorithm to build the Main chain (3)
 an algorithm for k peers selection via cost function (4) faster consensus
 selection via k peer broadcasts (5) data pruning via root creation 
 8
 The rest of this paper is organised as follows Section 2 gives an overview
 of Blockchain related work as well as existing DAG based protocols Section 3
 describes our new Lachesis protocol Section 4 presents Lachesis consensus
 algorithm Several discussions about Lachesis protocols are presented in
 Section 5 Section 6 concludes with some future work Section 7 Proof
 of Byzantine fault tolerance is described in Section 7 1 In Section 7 2 we
 present responses to certain attacks with the Lachesis protocol and consensus
 algorithm 
 2 Related work
 2 1 Lamport timestamps
 Lamport [11] denes the happened before relation between any pair of
 events in a distributed system of machines The happened before relation 
 denoted by is dened without using physical clocks to give a partial
 ordering of events in the system The relation satises the following
 three conditions (1) If b and b0 are events in the same process and b comes
 before b0 then b b0 (2) If b is the sending of a message by one process and
 b0 is the receipt of the same message by another process then b b0 (3) If
 b b0 and b0 b00 then b b00 Two distinct events b and b0 are said to be
 concurrent if b 9 b0 and b0 9 b 
 The happens before relation can be viewed as a causality eect that
 b b0 implies event b may causally aect event b0 Two events are concurrent
 if neither can causally aect the other 
 Lamport introduces logical clocks which is a way of assigning a number
 to an event A clock Ci for each process Pi is a function which assigns a
 number Ci(b) to any event b 2 Pi The entire system of blocks is represented
 by the function C which assigns to any event b the number C(b) where
 C(b) = Cj(b) if b is an event in process Pj The Clock Condition states that
 for any events b b0 if b b0 then C(b) < C(b0) 
 To satises the Clock Condition the clocks must satisfy two conditions 
 First each process Pi increments Ci between any two successive events Sec 
 ond we require that each message m contains a timestamp Tm which equals
 the time at which the message was sent Upon receiving a message times 
 tamped Tm a process must advance its clock to be later than Tm 
 Given any arbitrary total ordering  of the processes the total ordering
 ) is dened as follows if a is an event in process Pi and b is an event
 in process Pj then b ) b0 if and only if either (i) Ci(b) < Cj(b0) or (ii)
 9
 C(b) = Cj(b0) and Pi  Pj The Clock Condition implies that if b b0 then
 b ) b0 
 2 2 Concurrent common knowledge
 In the Concurrent common knowledge (CCK) paper [20] they dene a
 model to reason about the concurrent common knowledge in asynchronous 
 distributed systems A system is composed of a set of processes that can
 communicate only by sending messages along a xed set of channels The
 network is not necessarily completely connected The system is asynchronous
 in the sense that there is no global clock in the system the relative speeds
 of processes are independent and the delivery time of messages is nite but
 unbounded 
 A local state of a process is denoted by sj
 i Actions are state transformers 
 an action is a function from local states to local states An action can be
 either a send(m) action where m is a message a receive(m) action and
 an internal action A local history hi of process i is a (possibly innite)
 sequence of alternating local states|beginning with a distinguished initial
 state|and actions We write such a sequence as follows hi = s0i
 1i
 ô€€€ s1i
 2i
 ô€€€ 
 s2i
 3i
 ô€€€ The notation of sj
 i (j
 i ) refers to the j th state (action) in process
 i s local history An event is a tuple hs  s0i consisting of a state an action 
 and a state The jth event in process i s history is ej
 i denoting hsjô€€€1
 i j
 i sj
 i i 
 An asynchronous system consists of the following sets 
 1 A set P = f1 Ng of process identiers where N is the total number
 of processes in the system 
 2 A set C  f(i j) s t i j 2 Pg of channels The occurrence of (i j) in
 C indicates that process i can send messages to process j 
 3 A set Hi of possible local histories for each process i in Proc 
 4 A set A of asynchronous runs Each asynchronous run is a vector of
 local histories one per process indexed by process identiers Thus 
 we use the notation a = hh1 h2 h3 hNi Constraints on the set A are
 described throughout this section 
 5 A set M of messages A message is a triple hi j Bi where i 2 Proc is
 the sender of the message j 2 Proc is the message recipient and B is
 the body of the message B can be either a special value (e g a tag to
 denote a special purpose message) or some proposition about the run
 (e g \i has reset variable X to zero ) or both We assume for ease of
 exposition only that messages are unique 
 10
 The set of channels C and our assumptions about their behavior induce
 two constraints on the runs in A First i cannot send a message to j unless
 (i j) is a channel Second if the reception of a message m is in the run 
 then the sending of m must also be in that run this implies that the network
 cannot introduce spurious messages or alter messages 
 The CCK model of an asynchronous system does not mention time 
 Events are ordered based on Lamport s happens before relation They use
 Lamport s theory to describe global states of an asynchronous system A
 global state of run a is an n vector of prexes of local histories of a one
 prex per process The happens before relation can be used to dene a con 
 sistent global state often termed a consistent cut as follows 
 Denition 2 1 (Consistent cut) A consistent cut of a run is any global state
 such that if exi
 ey
 j and ey
 j is in the global state then exi
 is also in the global
 state 
 A message chain of an asynchronous run is a sequence of messages m1 
 m2 m3 such that for all i receive(mi) send(mi+1) Consequently 
 send(m1) receive(m1) send(m2) receive(m2) send(m3) 
 2 3 Consensus algorithms
 In a consensus algorithm all participant nodes of a distributed network
 share transactions and agree integrity of the shared transactions [13] It is
 equivalent to the proof of Byzantine fault tolerance in distributed database
 systems [1 12] The Practical Byzantine Fault Tolerance (pBFT) allows all
 nodes to successfully reach an agreement for a block when a Byzantine node
 exists [3] 
 There are numerous consensus algorithms being proposed [4 9] Proof
 of Work (PoW) requires large amounts of computational work to generate
 the blocks [19] Proof of Stake (PoS) [25 14] use participants stakes and
 delegated participants stake to generate the blocks respectively Alternative
 schemes are proposed to improve algorithms using directed acyclic graphs
 (DAG) [16] These DAG based approaches utilize the graph structures to de 
 cide consensus blocks and connections are considered as vertexes and edges 
 respectively 
 2 4 DAG based Approaches
 IOTA [22] published a DAG based technology called Tangle The Tips
 concept was used to address scalability issues with the limitations of the
 11
 Internet of Things Also a nonce by using weight level was composed to
 achieve the transaction consensus by setting the user s diculty To solve the
 double spending problem and parasite attack they used the Markov Chain
 Monte Carlo (MCMC) tip selection algorithm which randomly selects the
 tips based on the size of the accumulated transaction weights However if
 a transaction con
 icts with another there is still a need to examine all past
 transaction history to nd the con
 ict 
 Byteball [5] uses an internal pay system called bytes This is used to
 pay for adding data to the distributed database Each storage unit is linked
 to each other that includes one or more hashes of earlier storage units In
 particular the consensus ordering is composed by selecting a single Main
 Chain which is determined as a root consisting of the most roots A majority
 of roots detects the double spend attempts through consensus time of Main
 Chain The fee is charged according to the size of the bytes and the list of
 all units should be searched and updated in the process of determining the
 roots 
 RaiBlocks [15] has been developed to improve high fees and slow trans 
 action processing It is a process of obtaining consensus through the balance
 weighted vote on con
 icting transactions Each node participating in the
 network becomes the principal and manages its data history locally How 
 ever since RaiBlocks generate transactions in a similar way to an anti spam
 tool of PoW all nodes must communicate to create transactions In terms of
 scalability there is a need for steps to verify the entire history of transactions
 when a new node is added 
 Hashgraph [2] is an asynchronous DAG based distributed ledger Each
 node is connected by its own ancestor and randomly communicates known
 events through a gossip protocol At this time any famous node can be
 determined by the see and strong see relationship at each round to reach
 consensus quickly They state that if more than 2/3 of the nodes reach
 consensus for an event it will be assigned consensus position 
 Con
 ux [17] is a DAG based Nakamoto consensus protocol Con
 ux is a
 fast scalable and decentralized block chain system that optimistically pro 
 cesses concurrent blocks without discarding any as forks The Con
 ux pro 
 tocol achieves consensus on a total order of the blocks The total order of the
 transactions is decided by all participants of the network Con
 ux can toler 
 ate up to half of the network as malicious while the BFT based approaches
 can only tolerate up to one third of malicious nodes 
 Parsec [21] proposes an algorithm for reaching consensus in the presence
 12
 of Byzantine faults in a randomly synchronous network Like Hashgraph [2] 
 it has no leaders no round robin no proof of work and reaches eventual con 
 sensus with probability one Unlike Hashgraph it can provide high speed
 even in the presence of faults Parsec algorithm reaches BFT consensus with
 very weak synchrony assumptions Messages are delivered with random de 
 lays such that the average delay is nite It allows up to one third Byzantine
 (arbitrary) failures 
 Phantom [24] is a PoW based protocol for a permissionless ledger that
 generalizes Nakamoto s blockchain to a DAG of blocks PHANTOM includes
 a parameter k to adjust the tolerance level of the protocol to blocks that were
 created concurrently which can be set to accommodate higher throughput It
 thus avoids the security scalability tradeo as in Satoshi s protocol PHAN 
 TOM uses a greedy algorithm on the DAG to distinguish between blocks
 by honest nodes and those by non cooperating nodes This distinction gives
 PHANTOM a robust total order of the blocks that is eventually agreed upon
 by all honest nodes 
 Similar to PHANTOM the GHOSTDAG protocol selects a k cluster 
 which induces a colouring of the blocks as Blues (blocks in the selected clus 
 ter) and Reds (blocks outside the cluster) However instead of searching for
 the largest k cluster GHOSTDAG nds a cluster using a greedy algorithm 
 Spectre [23] is a new protocol for the consensus core of cryptocurrencies 
 SPECTRE which is PoW based protocol relies on a data structure that gen 
 eralizes Nakamoto s blockchain into a DAG It remains secure from attackers
 with up to 50% of the computational power even under high throughput and
 fast conrmation times Sprectre protocol satises weaker properties than
 classic consensus requires In SPECTRE the order between any two trans 
 actions can be decided from transactions performed by honest users This is
 dierent from the conventional paradigm in which the order must be decided
 by all non corrupt nodes 
 Blockmania [6] is a mechanism to achieve consensus with several advan 
 tages over the more traditional pBFT protocol and its variants In Blockma 
 nia nodes in a quorum only emit blocks linking to other blocks irrespective of
 the consensus state machine The resulting directed acyclic graph of blocks
 (block DAG) is later interpreted to ensure consensus safety nality and live 
 liness The resulting system has communication complexity O(N2) even in
 the worse case and low constant factors | as compared to O(N4) for pBFT 
 13
 3 Generic framework of Lachesis Protocols
 This section describes the key concepts of our new family of Lachesis
 protocols 
 3 1 OPERA chain
 The core idea of Lachesis protocols is to use a DAG based structure 
 called OPERA chain for our consensus algorithm In Lachesis protocol a
 (participant) node is a server (machine) of the distributed system Each
 node can create messages send messages to and receive messages from other
 nodes The communication between nodes is asynchronous 
 Lachesis Protocol consists of event blocks including user information and
 edges between event blocks In Lachesis Protocol event blocks are created
 by a node after the node communicates information of OPERA chain with
 another node The OPERA chain is comprised of event blocks as vertices
 and block communication as edges 
 Let n be the number of participant nodes For consensus the algorithm
 examines whether an event block is shared with 2n=3 nodes where n is the
 number of all nodes Sharing an event block with 2n=3 nodes means that
 more than two thirds of all nodes in the OPERA chain knows the event block 
 3 2 Main chain
 For faster consensus we introduce the Main chain which is a special sub 
 graph of the OPERA chain To improve path search we propose to use a
 local hash table structure as a cache that is used to quickly determine the
 closest root to a event block 
 In the OPERA chain an event block is called a root if the event block is
 linked to more than two thirds of previous roots A leaf vertex is also a root
 itself With root event blocks we can keep track of \vital blocks that 2n=3
 of the network agree on 
 The Main chain | a core subgraph of OPERA chain plays the important
 role for ordering the event blocks The Main chain stores shortcuts to connect
 between the Atropos After the topological ordering is computed over all
 event blocks through Lachesis protocol Atropos blocks are determined and
 form the Main chain Figure 3 shows an example of Main chain composed
 of Atropos event blocks In particular the Main chain consists of Atropos
 blocks those are derived from root blocks and so are agreed by 2n=3 of the
 14
 network nodes Thus this guarantees that at least 2n=3 of nodes have come
 to consensus on this Main chain 
 Each participant node has a copy of the Main chain and can search con 
 sensus position of its own event blocks Each event block can compute its
 own consensus position by checking the nearest Atropos event block Assign 
 ing and searching consensus position are introduced in the consensus time
 selection section 
 The Main chain provides quick access to the previous transaction history
 to eciently process new coming event blocks From Main chain informa 
 tion about unknown participants or attackers can be easily viewed The Main
 chain can be used eciently in transaction information management by pro 
 viding quick access to new event blocks that have been agreed on by the
 majority of nodes In short the Main chain gives the following advantages 
 All event blocks or nodes do not need to store all information It is
 ecient for data management 
 Access to previous information is ecient and fast 
 Based on these advantages OPERA chain can respond strongly to e 
 cient transaction treatment and attacks through its Main chain 
 r
 r
 r
 r
 r
 r
 r r
 a
 a a
 a
 a
 a
 a
 r
 r
 Figure 3 An Example of Main chain
 15
 3 3 Lachesis Consensus Algorithm (LCA)
 Our Lachesis algorithm (LCA) is presented LCA is one of the consen 
 sus algorithms for solving the byzantine agreement problem In LCA the
 OPERA chain uses root Clotho and Atropos blocks to nd consensus time
 for event blocks Algorithm 1 shows the pseudo algorithm of a OPERA chain 
 The algorithm consists of two parts and runs them in parallel 
 In one part each node requests synchronization and creates an event
 block In line 3 a node runs the Node Selection Algorithm The Node
 Selection Algorithm returns the k IDs of other nodes to communicate with 
 In line 4 and 5 the node synchronizes the OPERA chain with other nodes 
 Line 6 runs the Event block creation at which step the node creates an event
 block and checks whether it is root Then the node broadcasts the created
 event block to other nodes in line 7 The step in this line is optional In line 8
 and 9 Clotho selection and Atropos time consensus algorithms are invoked 
 The algorithms determinte whether the specied root can be a Clotho assign
 the consensus time and then conrm the Atropos 
 The second part is to respond to synchronization requests In line 10 and
 11 the node receives a synchronization request and then sends its response
 about the OPERA chain 
 Algorithm 1 Main Procedure
 1 procedure Main Procedure
 2 loop 
 3 A B = k node Selection algorithm()
 4 Request sync to node A and B
 5 Sync all known events by Lachesis protocol
 6 Event block creation
 7 (optional) Broadcast out the message
 8 Root selection
 9 Clotho selection
 10 Atropos time consensus
 11 loop 
 12 Request sync from a node
 13 Sync all known events by Lachesis protocol
 3 4 Node Structure
 This section gives an overview of node structure in Lachesis 
 16
 Each node has a signature stamp height vector in degree vector 
 ag
 table root hash list and Main chain Signature stamp is the data structure
 for storing the hash value that indicates the most recently created event block
 by the node We call the most recently created event block the top event
 block The 
 ag table is a n dimensional vector If an event block e created by
 ith node can reach jth root then the jth value in the 
 ag table of e becomes
 1 (otherwise 0) Each node only maintains the 
 ag table of the top event
 block 
 Node A ã€® Signature Stamp
 ã€® Height Vector
 ã€® In degree Vector
 1 2 â€¦â€¦ n
 â„Ž â„Ž# â€¦â€¦ â„Ž$
 1 2 â€¦â€¦ n
 ð‘– ð‘–# â€¦â€¦ ð‘–$
 ð‘†ð‘–ð‘”ð‘›ð‘Žð‘¡ð‘¢ð‘Ÿð‘’2
 ã€® Root Hash List
 ð‘Ÿâ„Ž ð‘Ÿâ„Ž# ð‘Ÿâ„Ž3 â€¦â€¦
 ã€® Main Chain (Hash List of Atropos)
 ð‘Žâ„Ž ð‘Žâ„Ž# ð‘Žâ„Ž3 â€¦â€¦
 ã€® OPERA chain
 Figure 4 An Example of Node Structure
 Figure 4 shows an example of the node structure component of a node
 A In the gure the signatureA stores the hash value of the top event block
 of A Each value in the height vector is the number of event blocks created
 by other nodes respectively The value of hi is the number of event blocks
 created by the ith node Each value in the in degree vector is the number of
 edges from other event blocks created by other nodes to the top event block 
 17
 The root hash list is the data structure storing the hash values of the root 
 The Main chain is a data structure storing hash values of the Atropos blocks 
 The Main chain is used to nd event blocks with complete consensus The
 root Clotho and Atropos selection algorithm are introduced in Section 4 
 3 5 Event block creation
 In Lachesis protocol every node can create an event block Each event
 block refers to other event blocks Reference means that the event block
 stores the hash values of the other event blocks In a Lachesis protocol an
 event block refers to k neighbor event blocks under the conditions as follows 
 1 The reference event blocks are the top event blocks 
 2 One reference should be made to a self parent 
 3 The own top event block refers to at least k neighbor of other nodes 
 3 6 Topological ordering of events using Lamport timestamps
 Every node has a physical clock and it needs physical time to create an
 event block However for consensus Lachesis protocols relies on a logical
 clock for each node For the purpose we use Lamport timestamps [11]
 to determine the time ordering between event blocks in a asynchronous dis 
 tributed system 
 The Lamport timestamps algorithm is as follows 
 1 Each node increments its count value before creating an event block 
 2 When sending a message include its count value receiver should con 
 sider which sender s message is received and increments its count value 
 3 If current counter is less than or equal to the received count value from
 another node then the count value of the recipient is updated 
 4 If current counter is greater than the received count value from another
 node then the current count value is updated 
 We use the Lamport s algorithm to enforce a topological ordering of event
 blocks and uses it in Atropos selection algorithm 
 Since an event block is created based on logical time the sequence be 
 tween each event blocks is immediately determined Because the Lamport
 timestamps algorithm gives a partial order of all events the whole time or 
 dering process can be used for Byzantine fault tolerance 
 18
 f1 f2 f3 f4
 1
 1
 1
 1
 1
 2
 2
 3
 4
 3
 5
 5
 6
 7
 8
 7
 6
 7
 8
 9
 9
 10
 10
 10
 Figure 5 An example of Lamport timestamps
 3 7 Topological consensus ordering
 The sequential order of each event block is an important aspect for Byzan 
 tine fault tolerance In order to determine the pre and post sequence between
 all event blocks we use Atropos consensus time Lamport timestamp algo 
 rithm and the hash value of the event block 
 First when each node creates event blocks they have a logical times 
 tamp based on Lamport timestamp This means that they have a partial
 ordering between the relevant event blocks Each Clotho has consensus time
 to the Atropos This consensus time is computed based on the logical time
 nominated from other nodes at the time of the 2n/3 agreement 
 In the LCA each event block is based on the following three rules to
 reach an agreement 
 1 If there are more than one Atropos with dierent times on the same
 frame the event block with smaller consensus time has higher priority 
 2 If there are more than one Atropos having any of the same consensus
 time on the same frame determine the order based on the own logical
 time from Lamport timestamp 
 19
 1
 1â€™
 1â€™â€™â€™â€™
 1â€™â€™
 1â€™â€™â€™
 3
 3â€™
 5
 5â€™
 4
 2
 2â€™
 1 If consensus time of 3 3â€™
 has a prior ordering
 2 If consensus time of
 and if Lamport time of
 has a prior ordering
 3 If consensus time of
 and if Lamport time of
 has a prior ordering
 and if hash value of
 < consensus time of
 3
 1â€™ = consensus time of
 1â€™ < Lamport time of
 1â€™
 2 = consensus time of
 2 = Lamport time of
 2 < hash value of
 2
 We assume that Atropos are 3 and 3â€™
 2â€™
 2â€™
 2â€™
 2â€™
 2â€™
 Topological consensus ordering
 1 Smaller consensus time of Atropos prior ordering 
 2 if same consensus time smaller Lamport timestamp prior ordering 
 3 if same consensus time and same Lamport timestamp smaller hash value prior ordering 
 Figure 6 An example of topological consensus ordering
 3 When there are more than one Atropos having the same consensus
 time if the local logical time is same a smaller hash value is given
 priority through hash function 
 Figure 6 depicts an example of topological consensus ordering 
 Figure 7 shows the part of OPERA chain in which the nal consensus
 order is determined based on these 3 rules The number represented by each
 event block is a logical time based on Lamport timestamp 
 20
 1
 1â€™
 1â€™â€™â€™â€™
 1â€™â€™
 1â€™â€™â€™
 2â€™
 2
 3â€™
 4
 3
 5â€™
 5
 6
 7
 6â€™
 7â€™
 7â€™
 8
 8â€™
 9
 10
 Figure 7 An Example of time ordering of event blocks in OPERA chain
 Final topological consensus order containing the event blocks based on
 agreement for the apropos Based on each Atropos they will have dierent
 colors depending on their range 
 3 8 Peer selection algorithm
 In order to create an event block a node needs to select k other nodes 
 Lachesis protocols does not depend on how peer nodes are selected One
 simple approach can use a random selection from the pool of n nodes The
 other approach is to dene some criteria or cost function to select other peers
 of a node 
 Within distributed system a node can select other nodes with low com 
 munication costs low network latency high bandwidth high successful trans 
 action throughputs 
 4 Lachesis Consensus Protocol L0
 This section presents our new Lachesis Consensus Protocol L0 which is
 a specic example of the Lachesis class We describe the main ideas and
 algorithms used in the protocol 
 21
 4 1 Root Selection
 All nodes can create event blocks and an event block can be a root when
 satisfying specic conditions Not all event blocks can be roots First the
 rst created event blocks are themselves roots These leaf event blocks form
 the rst root set RS1 If there are total n nodes and these nodes create the
 event blocks then the cardinality of the rst root set jRS1j is n Second if
 an event block e can reach at least 2n/3 roots then e is called a root This
 event e does not belong to RS1 but the next root set RS2 Thus excluding
 the rst root set the range of cardinality of root set RSk is 2n=3 < jRSkj  n 
 The event blocks including RSk before RSk+1 is in the frame fk The roots
 in RSk+1 does not belong to the frame fk Those are included in the frame
 fk + 1 when a root belonging to RSk+2 occurs 
 We introduce the use of a 
 ag table to quickly determine whether a new
 event block becomes a root or not Each node maintains a 
 ag table of the
 top event block Every event block that is newly created is assigned k hashes
 for its k parent event blocks We apply an OR operation on the 
 ag tables
 of the parent event blocks 
 Figure 8 shows an example of how to use 
 ag tables to determine a root 
 In this example r1 is the most recently created event block We apply an
 OR operation on the 
 ag tables of r1 s k parent event blocks The result is
 the 
 ag table of r1 If r1 s 
 ag table has more than 2n=3 set bits r1 is a
 root In this example the number of set bits is 4 which is greater than 2n=3
 (n=5) Thus r1 becomes root 
 The root selection algorithm is as follows 
 1 The rst event blocks are considered as root 
 2 When a new event block is added in the OPERA chain we check
 whether the event block is a root by applying OR operation on the
 
 ag tables connected to the new event block If the sum of the 
 ag
 table for the new event block is more than 2n/3 the new event block
 becomes a root 
 3 When a new root appears on the OPERA chain nodes update their
 root hash list If one of new event blocks becomes a root all nodes
 that share the new event block add the hash value of the event block
 to their root hash list 
 4 The new root set is created if the cardinality of previous root set RSp
 is more than 2n/3 and the new event block can reach 2n/3 root in RSp 
 22
 Ã§
 r0
 r0
 r0
 r0
 r0
 r1
 Flag table information
 1 1 1 0 0
 Flag table information
 0 1 1 1 0
 Flag table information
 Flag table information 1 1 1 1 0
 0 0 0 1 0
 r0 r0 r0 r0 r0
 1
 2
 3
 1 OR 2 OR 3
 Root set (Rs)
 r0 r0 r0 r0 r0
 r0 r0 r0 r0 r0
 r0 r0 r0 r0 r0
 Figure 8 An Example of Flag Table Calculation
 5 When the new root set RSk+1 is created the event blocks from previous
 root set RSk to before RSk+1 belong to the frame fk 
 4 2 Clotho Selection
 A Clotho is a root that satises the Clotho creation conditions Clotho
 creation conditions are that more than 2n/3 nodes know the root and a root
 knows this information 
 Figure 9 shows an example of Clotho Circles with a label ri (or c)
 represents a root (or Clotho) event block If there are three other sets of root
 and there exists one root after the recent clotho set then one of the roots in
 the rst root set become Clotho 
 Clotho selection algorithm checks whether root event blocks in the root
 hash list satisfy the Clotho condition If a root satises Clotho condition 
 the root becomes Clotho and makes a candidate time for Atropos After the
 root is concluded as a Clotho Atropos consensus time selection algorithm is
 triggered 
 For a root r we denote frame(i r) to be the root r in i th frame For
 example frame(1 r) is the rst root belong to the frame f1 
 23
 f1 f2
 c
 r0
 r0
 f3 f4
 r0
 r0
 r1
 r1
 r1
 r1
 r1 r2
 r2
 r2 r3
 r3
 r2
 r3
 r2
 r3
 Figure 9 An Example of Clotho
 Algorithm 2 shows the pseudo code for Clotho selection The algorithm
 takes a root r as input Line 4 and 5 set c is clotho and c yes to nil and 0
 respectively Line 6 8 checks whether any root c0 in frame(i ô€€€ 3 r) shares c
 where i is the current frame In line 9 10 if number of roots in frame(iô€€€2 r)
 which shares c is more than 2n=3 the root c is set as a Clotho The time
 complexity of Algorithm 3 is O(n2) where n is the number of nodes 
 4 3 Atropos Selection
 Atropos selection algorithm is the process in which the candidate time
 generated from Clotho selection is shared with other nodes and each root
 re selects candidate time repeatedly until all nodes have same candidate time
 for a Clotho 
 After a Clotho is nominated each node then computes candidate time
 of the Clotho If there are more than two thirds of the nodes that compute
 the same value for candidate time that time value is recorded Otherwise 
 each node reselects candidate time from some candidate time which the node
 collects By the reselection process each node reaches time consensus for can 
 didate time of Clotho as OPERA chain grows The candidate time reaching
 the consensus is called Atropos consensus time After Atropos consensus
 24
 Algorithm 2 Clotho Selection
 1 procedure Clotho Selection
 2 Input a root r
 3 for c 2 frame(i ô€€€ 3 r) do
 4 c is clotho nil
 5 c yes 0
 6 for c0 2 frame(i ô€€€ 2 r) do
 7 if c0 share c then
 8 c yes c yes + 1
 9 if c yes > 2n=3 then
 10 c is clotho yes
 time is computed Clotho is nominated to Atropos and each node stores
 the hash value of Atropos and Atropos consensus time in Main Chain The
 Main chain is used for time order between event blocks The proof of Atropos
 consensus time selection is shown in the section 7 1 
 25
 Algorithm 3 Atropos Consensus Time Selection
 1 procedure Atropos Consensus Time Selection
 2 Input c Clotho in frame fi
 3 c consensus time nil
 4 m the index of the last frame fm
 5 for d in [3 m] do
 6 R be the Root set RSi+d in frame fi+d
 7 for r 2 R do
 8 if d is 3 then
 9 if r conrms c as Clotho then
 10 r time(c) r lamport time
 11 else if d > 3 then
 12 s the set of Root in fjô€€€1 that r can share
 13 t RESELECTION(s c)
 14 k the number of root having t in s
 15 if d mod h > 0 then
 16 if k > 2n/3 then
 17 c consensus time t
 18 r time(c) t
 19 else
 20 r time(c) t
 21 else
 22 r time(c) the minimum value in s
 Algorithm 3 and 4 show pseudo code of Atropos consensus time selection
 and Consensus time reselection In Algorithm 3 at line 5 d denotes the
 deference of relationship between root set of c and r Thus line 7 means
 that r is one of the elements in root set of the frame fi+3 where the frame fi
 includes c Line 10 each root in the frame fj selects own Lamport timestamp
 as candidate time of c when they conrm root c as Cltoho In line 12 13 
 and 14 s t and k save the set of root that r can share c the result of
 RESELECTION function and the number of root in s having t Line
 15 is checking whether there is a dierence as much as h between i and
 j where h is a constant value for minimum selection frame Line 16 20 is
 checking whether more than two thirds of root in the frame fjô€€€1 nominate
 the same candidate time If two thirds of root in the frame fjô€€€1 nominate
 the same candidate time the root c is assigned consensus time as t Line 22
 26
 is minimum selection frame In minimum selection frame minimum value
 of candidate time is selected to reach byzantine agreement Algorithm 4
 operates in the middle of Algorithm 3 In Algorithm 4 input is a root set R
 and output is a reselected candidate time Line 4 5 computes the frequencies
 of each candidate time from all the roots in R In line 6 11 a candidate time
 which is smallest time that is the most nomitated The time complexity of
 Algorithm 4 is O(n) where n is the number of nodes Since Algorithm 3
 includes Algorithm 4 the time complexity of Algorithm 3 is O(n2) where n
 is the number of nodes 
 Algorithm 4 Consensus Time Reselection
 1 function Reselection
 2 Input Root set R and Clotho c
 3 Output candidate time t
 4  set of all ti = r time(c) for all r in R
 5 D set of tuples (ti ci) computed from  where ci = count(ti)
 6 max count max(ci)
 7 t infinite
 8 for tuple (ti ci) 2 D do
 9 if max count == ci && ti < t then
 10 t ti
 11 return t
 In the Atropos Consensus Time Selection algorithm nodes reach con 
 sensus agreement about candidate time of a Clotho without additional com 
 munication (i e exchanging candidate time) with each other Each node
 communicates with each other through the Lachesis protocol the OPERA
 chain of all nodes grows up into same shape This allows each node to know
 the candidate time of other nodes based on its OPERA chain and reach a
 consensus agreement The proof that the agreement based on OPERA chain
 become agreement in action is shown in the section 7 1 
 4 4 Peer selection algorithm via Cost function
 We dene three versions of the Cost Function (CF ) Version one is focused
 around updated information share and is discussed below The other two
 versions are focused on root creation and consensus facilitation these will be
 discussed in a following paper 
 27
 Leaf events
 A
 B
 C
 D
 E
 
 Node A
 ã€® Signature Stamp
 ã€® Height Vector
 ã€® In degree Vector
 A B C D E
 1 1 1 1 1
 #$%& ()*+ 
 A B C D E
 0 0 0 0 0
 Cost Function = 0/2
 Leaf events
 A
 B
 C
 D
 E
 
 Node A
 ã€® Signature Stamp
 ã€® Height Vector
 ã€® In degree Vector
 A B C D E
 1 1 1 1 1
 #$%& ()*+ 
 A B C D E
 0 0 0 0 0
 Cost Function = 0/2
 34(B)=0 34(C)=0 34(D)=0 34(E)=0
 Step 1 Step 2
 Figure 10 An Example of Cost Function 1
 We dene a Cost Function (CF ) for preventing the creation of lazy nodes 
 The lazy node is a node that has a lower work portion in the OPERA chain 
 When a node creates an event block the node selects other nodes with low
 values outputs from the cost function and refers to the top event blocks of
 the reference nodes An equation (1) of CF is as follows 
 CF = I=H (1)
 where I and H denote values of in degree vector and height vector re 
 spectively If the number of nodes with the lowest CF is more than k one
 of the nodes is selected at random The reason for selecting high H is that
 we can expect a high possibility to create a root because the high H indi 
 cates that the communication frequency of the node had more opportunities
 than others with low H Otherwise the nodes that have high CF (the case
 of I > H) have generated fewer event blocks than the nodes that have low
 CF Then we can judge that those kind of nodes are lazy If we can detect
 whether a node is lazy based on cost function we can change the lazy nodes
 to other participants or remove them 
 Figure 10 shows an example of the node selection based on the cost func 
 tion after the creation of leaf events by all nodes In this example there
 are ve nodes and each node created leaf events All nodes know other leaf
 events Node A creates an event block v1 and A calculates the cost functions 
 Step 2 in Figure 10 shows the results of cost functions based on the height
 28
 Leaf events
 A
 B
 C
 D
 E
 
 Node A ã€® Signature Stamp
 ã€® Height Vector
 ã€® In degree Vector
 A B C D E
 3 2 1 1 1
 #$%& ()*+ 
 A B C D E
 0 1 1 1 1
 Cost Function = 0/2
 3
 4
 5
 6
 78(B)=0 5 78(C)=1 78(D)=1 78(E)=1
 Figure 11 An Example of Cost Function 2
 and in degree vectors of node A In the initial step each value in the vectors
 are same because all nodes have only leaf events Node A randomly selects k
 nodes and connects v1 to the leaf events of selected nodes In this example 
 we set k=3 and assume that node A selects node B and C 
 Figure 11 shows an example of the node selection after a few steps of the
 simulation in Figure 10 In Figure 11 the recent event block is v5 created by
 node A Node A calculates the cost function and selects the other two nodes
 that have the lowest results of the cost function In this example node B
 has 0 5 as the result and other nodes have the same values Because of this 
 node A rst selects node B and randomly selects other nodes among nodes
 C D and E 
 The height of node D in the current OPERA chain of the example is 2
 (leaf event and event block v4) On the other hand the height of node D
 in node structure of A is 1 Node A is still not aware of the presence of the
 event block v4 It means that there is no path from the event blocks created
 by node A to the event block v4 Thus node A has 1 as the height of node
 D 
 Algorithm 5 shows the selecting algorithm for selecting reference nodes 
 The algorithm operates for each node to select a communication partner
 from other nodes Line 4 and 5 set min cost and Sref to initial state Line
 7 calculates the cost function cf for each node In line 8 9 and 10 we nd
 29
 the minimum value of the cost function and set min cost and Sref to cf and
 the ID of each node respectively Line 11 and 12 append the ID of each node
 to Sref if min cost equals cf Finally line 13 selects randomly k node IDs
 from Sref as communication partners The time complexity of Algorithm 2
 is O(n) where n is the number of nodes 
 Algorithm 5 k neighbor Node Selection
 1 procedure k node Selection
 2 Input Height Vector H In degree Vector I
 3 Output reference node ref
 4 min cost INF
 5 sref None
 6 for k 2 Node Set do
 7 cf Ik
 Hk
 8 if min cost > cf then
 9 min cost cf
 10 sref k
 11 else if min cost equal cf then
 12 sref sref [ k
 13 ref random select in sref
 After the reference node is selected each node communicates and shares
 information that is all event blocks known by them A node creates an event
 block by referring to the top event block of the reference node The Lachesis
 protocol works and communicates asynchronously This allows a node to
 create an event block asynchronously even when another node creates an
 event block The communication between nodes does not allow simultaneous
 communication with the same node 
 Figure 12 shows an example of the node selection in Lachesis protocol 
 In this example there are ve nodes (A B C D and E) and each node
 generates the rst event blocks called leaf events All nodes share other leaf
 events with each other In the rst step node A generates new event block
 v1 (blue) Then node A calculates the cost function to connect other nodes 
 In this initial situation all nodes have one event block called leaf event thus
 the height vector and the in degree vector in node A has same values In
 other words the heights of each node are 1 and in degrees are 0 Because
 of this reason node A randomly select other two nodes and connect v1 to
 30
 Leaf events
 A
 B
 C
 D
 E
 Leaf events
 A
 B
 C
 D
 E
 Step 1 Step 2
 
 Leaf events
 A
 B
 C
 D
 E
 Leaf events
 A
 B
 C
 D
 E
 Step 3 Step 4
 
 # #
 Figure 12 An Example of Node Selection
 the top two event blocks by other two nodes The step 2 shows the situation
 after connections In this example node A select node B and C to connect
 v1 and the event block v1 is connected to the top event blocks of node B and
 C Node A only knows the situation of the step 2 
 After that in the example node B generates new event block v2 (green)
 and also calculates the cost function B randomly select the other two nodes 
 A and D since B only has information of the leaf events Node B requests
 to A and D to connect eB then nodes A and D send information for their
 top event blocks to node B as response The top event block of node A is v1
 and node D is the leaf event The event block v2 is connected to v1 and leaf
 event from node D Step 4 shows these connections 
 5 Discussions
 This section presents several discussions on our Lachesis protocol 
 5 1 Lamport timestamps
 This section discusses a topological order of event blocks in DAG based
 Lachesis protocols using Lamport timestamps [11] 
 Our Lachesis protocols relies on Lamport timestamps to dene a topo 
 logical ordering of event blocks in OPERA chain The \happened before 
 relation denoted by gives a partial ordering of events from a distributed
 system of nodes 
 Given n nodes they are represented by n processes P = (P0 P1 Pnô€€€1) 
 31
 For a pair of event blocks b and b0 the relation satises (1) If b and
 b0 are events of process Pi and b comes before b0 then b b0 (2) If b is
 the send(m) by one process and b0 is the receive(m) by another process then
 b b0 (3) If b b0 and b0 b00 then b b00 Two distinct events b and b0
 are said to be concurrent if b 9 b0 and b0 9 b 
 For an arbitrary total ordering  of the processes a relation ) is dened
 as follows if b is an event in process Pi and b0 is an event in process Pj 
 then b ) b0 if and only if either (i) Ci(a) < Cj(b) or (ii) C(b) = Cj(b0) and
 Pi  Pj This denes a total ordering and that the Clock Condition implies
 that if a b then a ) b 
 We use this total ordering in our Lachesis algorithms By using Lamport
 timestamps we do not rely on physical locks to determine a partial ordering
 of events 
 5 2 Semantics of Lachesis protocols
 This section discusses the possible usage of concurrent common knowl 
 edge described in Section 2 2 to understand DAG based consensus protocols 
 Let G = (V E) denote directed acyclic graph (DAG) V is a set of vertices
 and E is a set of edges DAG is a directed graph with no cycle Namely in
 DAG there is no path that source and destination at the same vertex A
 path is a sequence of vertices (v1 v2 v(k 1) vk) that uses no edge more
 than once 
 An asynchronous system consists of the following sets 
 1 A set P = f1 ng of process identiers where n is the total number
 of processes Pi in the system 
 2 A set C  f(i j) s t i j 2 Pg of channels If (i j) in C it indicates
 that process i can send messages to process j 
 3 A set Hi of possible local histories for each process i in P 
 4 A set A of asynchronous runs Each asynchronous run is a vector of
 local histories denoted by a = hh1 h2 h3 hNi Each process has a
 single run Histories are indexed by process identiers 
 5 A set M of messages A message is a triple hi j Bi where i 2 P is
 the sender of the message j 2 is the message recipient and B is the
 message body 
 In Lachesis protocol each node selects k other nodes as peers For certain
 gossip protocol nodes may be constrained to gossip with its k peers In such
 32
 a case the set of channels C can be modelled as follows If node i selects
 node j as a peer then (i j) 2 C In general one can express the history of
 each node in Lachesis protocol in the same manner as in the CCK paper [20] 
 Thus a proof of consensus can be formalized via the consistent cuts 
 6 Conclusion
 In order to realize the distributed ledger technology we have proposed a
 new family of asynchronous DAG based consensus protocol namely L We
 introduce the OPERA chain and Main chain for faster consensus By using
 Lamport timestamps the topological ordering of event blocks in OPERA
 chain and Main chain is more intuitive and reliable in distributed system 
 We introduce a 
 ag table at each block to improve root detection 
 Further we have presented a specic Lachesis consensus protocol called
 L0 as an example of L The L0 protocol uses a new 
 ag table in each block
 as a shortcut to check for reachability from an event block to a root along
 the OPERA chain The path search is used as a proof of pBFT consensus 
 In terms of eectiveness using 
 ag table in L0 protocol is more eective
 for consensus compared to the path searching approaches To ensure the
 distribution of participating nodes the Lachesis protocol denes a new cost
 function and an algorithm that eciently and quickly selects peers We also
 propose new algorithms for root selection and Clotho block selection based on
 the 
 ag table for Atropos selection by Weight after time consensus ordering 
 Based on the L0 protocol and the new consensus algorithm the OPERA
 chain can protect against malicious attacks such as forks double spending 
 parasite chains and network control These protections guarantee the safety
 of OPERA chain We can also verify existence of Atropos with the OPERA
 chain It concludes that the OPERA chain reaches consensus and guarantees
 liveliness Finally the time ordering ensures guarantee by weight value on the
 
 ag table Based on these properties the LCA provides a fair transparent 
 and eective consensus algorithm 
 6 1 Future work
 There are a number of directions for future work 
  With the Lachesis protocols we are investigating a fast node synchro 
 nization algorithm with k neighbor broadcasts With OPERA chain
 and k peer selection it is possible to achieve a faster gossip broadcast 
 33
 We are interested in comparing performance of dierent gossip strate 
 gies such as randomized gossip broadcast gossip and collection tree
 protocol for distributed averaging in wireless sensor networks 
  We are also investigating the semantics of DAG based protocols in gen 
 eral and Lachesis protocols in particular We aim to have a formal proof
 of pBFT using concurrent common knowledge via consistent cuts 
 34
 7 Appendix
 7 1 Proof of Lachesis Consensus Algorithm
 In this section we provide proof of liveness and safety in OPERA chain
 and show the Byzantine fault tolerance To represent the Byzantine fault
 tolerance we assume that more than two thirds of participants are reliable
 nodes Based on the assumption we provide some denitions lemmas and
 theorems Then we eventually validate the Byzantine fault tolerance 
 7 1 1 Preliminaries
 Let G = (V E) denote directed acyclic graph (DAG) V is a set of vertices
 and E is a set of edges DAG is a directed graph with no cycle Namely in
 DAG there is no path that source and destination at the same vertex A
 path is a sequence P of vertices (v1 v2 v(k 1) vk) that uses no edge more
 than once Suppose that we have a current vertex vc and current event block
 ec respectively A vertex vp is parent of vc if there is a path from vc to vp and
 the length of path is 1 A vertex va is ancestor of vc if there is a path from
 vc to va and the length of path is more than equal to 1 
 7 1 2 Proof of Byzantine Fault Tolerance for Lachesis Consensus Algorithm
 Denition 7 1 (node) The machine that participates in the OPERA chain
 and creates event blocks The total number of nodes is n 
 Denition 7 2 (event block) In OPERA chain we call a vertex an event
 block 
 Denition 7 3 (self parent) An event block es is self parent of an event
 block ec if es is parent of ec and both event blocks have same signatures 
 Denition 7 4 (self ancestor) An event block ea is self ancestor of an event
 block ec if ea is ancestor of ec and both event blocks have same signatures 
 Denition 7 5 (Happened Before) An event block ex Happened Before an
 event block ey if there is a path from ex to ey 
 Denition 7 6 (Root) The rst created event blocks (leaf events) become
 root or an event block e that can reach more than 2n/3 other roots becomes
 a root 
 35
 Denition 7 7 (Root set) All rst event blocks (leaf events) are elements
 of root set R1 (jR1j = n) And the root set Rk is a set of roots such that ri
 2 Rk cannot reach more than 2n/3 other roots in Rk (k > 1) 
 Denition 7 8 (Frame) Frame f is a natural number that separates Root
 sets 
 Denition 7 9 (Clotho) A root rk in the frame fa+3 can nominate a root
 ra as Clotho if more than 2n/3 roots in the frame fa+1 Happened Before ra
 and rk Happened Before the roots in the frame fa+1 
 Denition 7 10 (Atropos) If the consensus time of Clotho is validated the
 Clotho become Atropos 
 Proposition 7 1 At least 2n/3 roots in the frame fi Happened Before at
 least 2n/3 roots in the frame fi+1 
 Proof The number of roots in each root set is more than 2n/3 Since a root
 in the frame fi+1 Happened Before more than 2n/3 roots in the frame fi 
 when the cardinalities of the root sets in the frames fi and fi+1 are n and
 2n/3 respectively the number of paths from root set in the frame fi+1 to root
 set in the frame fi is at least (2n/3)2 The average and the maximum of the
 number of paths from root set in the frame fi+1 to an root in the frame fi
 are (4n/9) and (2n/3) respectively Thus at least 2n/3 roots in the frame fi
 Happened Before at least n/3 root in the frame fi+1 
 Proposition 7 2 If a root in the frame fi Happened Before from more than
 n/3 roots in the frame fi+1 the root Happened Before all roots in the frame
 fi+2 
 Proof Based on the denition of Root each root can reach more than 2n/3
 other roots in the previous frame It means that a root in the frame fi+2
 should have a number of paths more than 2n/3 to roots in the frame fi+1 
 Thus if a root r in the frame fi Happened Before more than n/3 roots in
 the frame fi+1 all roots in the frame fi+2 have path to the root r 
 Lemma 7 3 (Sharing) If a root ra in the frame fa+3 is created the root
 in the frame fa+3 knows that more than 2n/3 roots in the frame fa become
 known by more than 2n/3 nodes 
 36
 Proof Based on propositions 7 1 and 7 2 the root in the frame fa+3 knows
 that more than 2n/3 roots in the frame fa become known by more than 2n/3
 nodes 
 Lemma 7 4 (Fork) If the pair of event blocks (x y) is a fork roots happened 
 before at least one fork in OPERA chain Therefore they can know fork
 before becoming Clotho 
 Proof Suppose that a node creates two event blocks (x y) and the event
 blocks are a fork To create two Clotho that can reach each event block in the
 pair the event blocks should be shared in more than 2n/3 nodes Therefore 
 if there exist fork event blocks the OPERA chain can structurally detect the
 fork before roots become Clotho 
 Theorem 7 5 All node grows up into same shape in OPERA chain 
 Proof Suppose that each node A and B will have a dierent shape (or a
 structure) For any two nodes A and B there is two event blocks x and y
 which are in both OPERA(A) and OPERA(B) Also their path between x
 and y in OPERA(A) is not equal to that in OPERA(B) For any two event
 blocks if each node has dierent paths we can consider that the dierence
 is fork attacks Based on Lemma 7 4 if an attacker forks an event block 
 the OPERA chain can detect and remove it before the Clotho is generated 
 It contradicts our assumptions For this reason two nodes have consistent
 OPERA chain 
 Lemma 7 6 For any root set R all nodes nominate same root into Clotho 
 Proof Based on Theorem 7 5 each node nominates a root into Clotho via
 the 
 ag table If all nodes have an OPERA chain with same shape the values
 in 
 ag table should be equal to each other in OPERA chain Thus all nodes
 nominate the same root into Clotho since the OPERA chain of all nodes has
 same shape 
 Lemma 7 7 In the Reselection algorithm for any Clotho a root in OPERA
 chain selects the same consensus time candidate 
 Proof Based on Theorem 7 5 if all nodes have an OPERA chain with the
 same partial shape a root in OPERA chain selects the same consensus time
 candidate by the Reselection algorithm 
 37
 Theorem 7 8 Lachesis consensus algorithm guarantees to reach agreement
 for the consensus time 
 Proof For any root set R in the frame fi time consensus algorithm checks
 whether more than 2n/3 roots in the frame fiô€€€1 selects the same value How 
 ever each node selects one of the values collected from the root set in the
 previous frame by the time consensus algorithm and Reselection process 
 Based on the Reselection process the time consensus algorithm can reach
 agreement However there is a possibility that consensus time candidate
 does not reach agreement [7] To solve this problem time consensus algo 
 rithm includes minimal selection frame per next h frame In minimal value
 selection algorithm each root selects minimum value among values collected
 from previous root set Thus the consensus time reaches consensus by time
 consensus algorithm 
 Theorem 7 9 If the number of reliable nodes is more than 2n/3 event
 blocks created by reliable nodes must be assigned to consensus order 
 Proof In OPERA chain since reliable nodes try to create event blocks by
 communicating with every other nodes continuously reliable nodes will share
 the event block x with each other Based on Proposition 7 1 if a root y in
 the frame fi Happened Before event block x and more than 2n/3 roots in
 the frame fi+1 Happened Before the root y the root y will be nominated
 as Clotho and Atropos Thus event block x and root y will be assigned
 consensus time t 
 For an event block assigning consensus time means that the validated
 event block is shared by more than 2n/3 nodes Therefore malicious node
 cannot try to attack after the event blocks are assigned consensus time When
 the event block x has consensus time t it cannot occur to discover new event
 blocks with earlier consensus time than t There are two conditions to be
 assigned consensus time earlier than t for new event blocks First a root r
 in the frame fi should be able to share new event blocks Second the more
 than 2n/3 roots in the frame fi+1 should be able to share r Even if the rst
 condition is satised by malicious nodes (e g parasite chain) the second
 condition cannot be satised since at least 2n/3 roots in the frame fi+1 are
 already created and cannot be changed Therefore after an event block is
 validated new event blocks should not be participate earlier consensus time
 to OPERA chain 
 38
 7 2 Response to Attacks
 Like all other decentralized blockchain technologies OPERA chain will
 likely be subject to attacks by attackers which aim to gain nancial prot to
 damage the system Here we describe several possible attack scenarios and
 how the OPERA chain intends to take preventive measures 
 7 2 1 Transaction Flooding
 A malicious participant may run a large number of valid transactions
 from their account under their control with the purpose of overloading the
 network In order to prevent such a case the chain intends to impose a
 minimal transaction fee Since there is a transaction fee the malicious user
 cannot continue to perform such attacks Participants who participate in
 nodes are rewarded and those who contribute to the ecosystem such as by
 running transactions are continuously rewarded Such rewards are expected
 to be adequate in running transactions for appropriate purposes However 
 since it would require tremendous cost to perform abnormal attacks it would
 be dicult for a malicious attacker to create transaction 
 ooding 
 7 2 2 Parasite chain attack
 In a DAG based protocol a parasite chain can be made with a malicious
 purpose attempting connection by making it look like a legitimate event
 block When the Main Chain is created verication for each event block is
 performed In the verication process any event block that is not connected
 to the Main Chain is deemed to be invalid and is ignored as in the case of
 double spending 
 We suppose that less than one third of nodes are malicious The malicious
 nodes create a parasite chain By the root denition roots are nominated by
 2n/3 node awareness A parasite chain is only shared with malicious nodes
 that are less than one third of participating nodes A parasite chain is unable
 to generate roots and have a shared consensus time 
 7 2 3 Double Spending
 A double spend attack is when a malicious entity attempts to spend their
 funds twice Entity A has 10 tokens they send 10 tokens to B via node nA
 and 10 tokens to C via node nZ Both node nA and node nZ agree that the
 transaction is valid since A has the funds to send to B (according to nA)
 and C (according to nZ) 
 39
 Consensus is a mechanism whereby multiple distributed parties can reach
 agreement on the order and state of a sequence of events Let s consider the
 following 3 transactions 
 txA A (starting balance of 10) transfers 10 to B
 txB B (starting balance of 0) transfers 10 to C
 txC C (starting balance of 0) transfers 10 to D
 We consider Node nA received the order txA txB txC
 The state of Node nA is A 0 B 0 C 0 D 10
 Now we consider Node nB that receives the order txC txB txA
 The state of Node nB is A 0 B 10 C 0 D 0
 Consensus ordering gives us a sequence of events 
 If the pair of event blocks (x y) has a double spending transaction the
 chain can structurally detect the double spend and delay action for the event
 blocks until the event blocks assign time ordering 
 Suppose that the pair of event blocks (x y) has same frame f1 Then 
 all nodes must detect two event blocks before frame f + 2 By the root
 denition each root happened before more than 2n=3 previous roots For
 this reason when two roots in f +1 are selected they must have happened 
 before the roots which are more than one thirds of roots in f This means
 that more than 2n=3 roots in f + 1 share both two roots which include the
 pair respectively With the root denition and previous explanation all roots
 in f + 2 share both the pairs Thus all nodes detect the double spending
 event blocks at f + 2 or earlier 
 7 2 4 Long range attack
 In blockchains an adversary can create another chain If this chain is
 longer than the original the network will accept the longer chain This mech 
 anism exists to identify which chain has had more work (or stake) involved
 in its creation 
 2n=3 participating nodes are required to create a new chain To accom 
 plish a long range attack you would rst need to create > 2n=3 participating
 malicious nodes to create the new chain 
 7 2 5 Bribery attack
 An adversary could bribe nodes to validate con
 icting transactions Since
 2n=3 participating nodes are required this would require the adversary to
 bribe > 1n=3 of all nodes to begin a bribery attack 
 40
 7 2 6 Denial of Service
 LCA is a leaderless system requiring 2n=3 participation An adversary
 would have to deny > 1n=3 participants to be able to successfully mount a
 DDoS attack 
 7 2 7 Sybil
 Each participating node must stake a minimum amount of FTM to par 
 ticipate in the network Being able to stake 2n=3 total stake would be pro 
 hibitively expensive 
 41
 8 Reference
 [1] J Aspnes Randomized protocols for asynchronous consensus Dis 
 tributed Computing 16(2 3) 165{175 2003 
 [2] L Baird Hashgraph consensus fair fast byzantine fault tolerance 
 Technical report 2016 
 [3] M Castro and B Liskov Practical byzantine fault tolerance In Proceed 
 ings of the Third Symposium on Operating Systems Design and Imple 
 mentation OSDI 99 pages 173{186 Berkeley CA USA 1999 USENIX
 Association 
 [4] J Chen and S Micali Algorand arXiv preprint arXiv 1607 01341 
 2016 
 [5] A Churyumov Byteball A decentralized system for storage and trans 
 fer of value 2016 
 [6] G Danezis and D Hrycyszyn Blockmania from block dags to consen 
 sus 2018 
 [7] M J Fischer N A Lynch and M Paterson Impossibility of distributed
 consensus with one faulty process J ACM 32(2) 374{382 1985 
 [8] M J Fischer N A Lynch and M S Paterson Impossibility of
 distributed consensus with one faulty process Journal of the ACM
 (JACM) 32(2) 374{382 1985 
 [9] Y Gilad R Hemo S Micali G Vlachos and N Zeldovich Algorand 
 Scaling byzantine agreements for cryptocurrencies In Proceedings of the
 26th Symposium on Operating Systems Principles pages 51{68 ACM 
 2017 
 [10] R Kotla L Alvisi M Dahlin A Clement and E Wong Zyzzyva 
 speculative byzantine fault tolerance ACM SIGOPS Operating Systems
 Review 41(6) 45{58 2007 
 [11] L Lamport Time clocks and the ordering of events in a distributed
 system Communications of the ACM 21(7) 558{565 1978 
 42
 [12] L Lamport et al Paxos made simple ACM Sigact News 32(4) 18{25 
 2001 
 [13] L Lamport R Shostak and M Pease The byzantine generals problem 
 ACM Trans Program Lang Syst 4(3) 382{401 July 1982 
 [14] D Larimer Delegated proof of stake (dpos) 2014 
 [15] C LeMahieu Raiblocks A feeless distributed cryptocurrency network 
 2017 
 [16] S D Lerner Dagcoin 2015 
 [17] C Li P Li W Xu F Long and A C c Yao Scaling nakamoto
 consensus to thousands of transactions per second arXiv preprint
 arXiv 1805 03870 2018 
 [18] A Miller Y Xia K Croman E Shi and D Song The honey badger
 of bft protocols In Proceedings of the 2016 ACM SIGSAC Conference
 on Computer and Communications Security pages 31{42 ACM 2016 
 [19] S Nakamoto Bitcoin A peer to peer electronic cash system 2008 
 [20] P Panangaden and K Taylor Concurrent common knowledge dening
 agreement for asynchronous systems Distributed Computing 6(2) 73{
 93 1992 
 [21] F H Q M S S Pierre Chevalier Bartomiej Kamin ski Protocol for
 asynchronous reliable secure and ecient consensus (parsec) 2018 
 [22] S Popov The tangle 2017 
 [23] Y Sompolinsky Y Lewenberg and A Zohar Spectre A fast and
 scalable cryptocurrency protocol IACR Cryptology ePrint Archive 
 2016 1159 2016 
 [24] Y Sompolinsky and A Zohar Phantom ghostdag Two scalable
 blockdag protocols 2008 
 [25] S N Sunny King Ppcoin Peer to peer crypto currency with proof of 
 stake 2012 
 [26] M Swan Blockchain Blueprint for a new economy O Reilly Media 
 2015 
 43 